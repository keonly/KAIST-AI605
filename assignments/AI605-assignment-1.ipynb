{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8324624d",
   "metadata": {},
   "source": [
    "# 2023 Fall AI605 Assignment 1: Recurrent Neural Networks\n",
    "\n",
    "## Rubric\n",
    "\n",
    "### Deadline\n",
    "\n",
    "The deadline for this assignment is: Friday 22nd September 2023 (Week 4) 11:59pm\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit your assignment via [KLMS](https://klms.kaist.ac.kr). You must submit the Jupyter Notebook file (.ipynb) with all code and model outputs.\n",
    "\n",
    "Use in-line LaTeX for mathematical expressions.\n",
    "\n",
    "### Collaboration\n",
    "\n",
    "This assignment is an individual assingnment. It is **not** a group assignment so make sure your answer and code are your own.\n",
    "\n",
    "### Grading\n",
    "\n",
    "The total number of marks avaiable is 25 points.\n",
    "\n",
    "### Environment\n",
    "\n",
    "The use of a GPU is not required for this notebook. The suggested environment for this is Python 3.9. Run the following cell to set up the environment.\n",
    "\n",
    "### Data\n",
    "\n",
    "All problems will use the stanford sentiment treebank dataset. However, pre-procesing it can take a bit of work https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip. To save time, we will use the Huggingface implementation of the dataset: https://huggingface.co/datasets/sst\n",
    "\n",
    "### Libraries\n",
    "\n",
    "The following libraries should be used for the project. You should not need any other libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32dcb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch tqdm datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c4769",
   "metadata": {},
   "source": [
    "# Problem 1 - MLP vs RNN (5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103700a1",
   "metadata": {},
   "source": [
    "This problem will make use of the stanford sentiment treebank dataset. We can load and partition the data with the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141b08ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48423b80e9fe4aa4a3556094a1c237ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9945ba8ea574da8ac7ac1db3be57e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/5.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9020fa52bc874812b120d5b2043a9c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2a4aef6f1b493a8f9f669adcf0c19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becce6526f4747c7b861bf7d9ee54448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/790k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d193fc47f914418a976f4003751d12e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8052a62aab2f4469b0db0f6328225b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3958dc9393db41729ccece5b5ff61266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Instance: {'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'label': 0.6944400072097778, 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n",
      "Tokens: ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.']\n",
      "Label: positive\n",
      "***\n",
      "\n",
      "Raw Instance: {'sentence': \"The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\", 'label': 0.833329975605011, 'tokens': \"The|gorgeously|elaborate|continuation|of|``|The|Lord|of|the|Rings|''|trilogy|is|so|huge|that|a|column|of|words|can|not|adequately|describe|co-writer\\\\/director|Peter|Jackson|'s|expanded|vision|of|J.R.R.|Tolkien|'s|Middle-earth|.\", 'tree': '71|70|69|69|67|67|66|64|63|62|62|61|61|58|57|57|56|53|53|52|52|49|49|50|48|45|44|43|43|42|42|41|39|38|38|40|60|39|40|41|47|46|44|45|46|47|48|51|50|51|55|54|54|55|56|59|58|59|60|73|65|63|64|65|66|68|68|72|70|71|72|73|0'}\n",
      "Tokens: ['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer\\\\/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.']\n",
      "Label: strong positive\n",
      "***\n",
      "\n",
      "Raw Instance: {'sentence': 'Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .', 'label': 0.625, 'tokens': 'Singer\\\\/composer|Bryan|Adams|contributes|a|slew|of|songs|--|a|few|potential|hits|,|a|few|more|simply|intrusive|to|the|story|--|but|the|whole|package|certainly|captures|the|intended|,|er|,|spirit|of|the|piece|.', 'tree': '72|71|71|70|68|68|67|67|66|63|62|62|60|60|58|58|56|55|55|54|53|53|65|75|51|50|50|49|48|46|44|43|42|42|45|41|40|40|77|41|47|43|44|45|46|47|48|49|52|51|52|76|54|57|56|57|59|59|61|61|64|63|64|65|66|74|69|69|70|73|72|73|74|75|76|77|0'}\n",
      "Tokens: ['Singer\\\\/composer', 'Bryan', 'Adams', 'contributes', 'a', 'slew', 'of', 'songs', '--', 'a', 'few', 'potential', 'hits', ',', 'a', 'few', 'more', 'simply', 'intrusive', 'to', 'the', 'story', '--', 'but', 'the', 'whole', 'package', 'certainly', 'captures', 'the', 'intended', ',', 'er', ',', 'spirit', 'of', 'the', 'piece', '.']\n",
      "Label: positive\n",
      "***\n",
      "\n",
      "Raw Instance: {'sentence': \"You 'd think by now America would have had enough of plucky British eccentrics with hearts of gold .\", 'label': 0.5, 'tokens': \"You|'d|think|by|now|America|would|have|had|enough|of|plucky|British|eccentrics|with|hearts|of|gold|.\", 'tree': '36|35|34|33|33|32|30|29|27|26|25|24|23|23|22|21|20|20|31|21|22|28|24|25|26|27|28|29|30|31|32|37|34|35|36|37|0'}\n",
      "Tokens: ['You', \"'d\", 'think', 'by', 'now', 'America', 'would', 'have', 'had', 'enough', 'of', 'plucky', 'British', 'eccentrics', 'with', 'hearts', 'of', 'gold', '.']\n",
      "Label: neutral\n",
      "***\n",
      "\n",
      "Raw Instance: {'sentence': 'Yet the act is still charming here .', 'label': 0.7222200036048889, 'tokens': 'Yet|the|act|is|still|charming|here|.', 'tree': '15|13|13|10|9|9|11|12|10|11|12|14|14|15|0'}\n",
      "Tokens: ['Yet', 'the', 'act', 'is', 'still', 'charming', 'here', '.']\n",
      "Label: positive\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from itertools import islice\n",
    "\n",
    "sst = datasets.load_dataset(\"sst\")\n",
    "\n",
    "\n",
    "def score_to_label(score):\n",
    "    if score < 0.2:\n",
    "        return \"strong negative\"\n",
    "    elif score < 0.4:\n",
    "        return \"negative\"\n",
    "    elif score < 0.6:\n",
    "        return \"neutral\"\n",
    "    elif score < 0.8:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"strong positive\"\n",
    "\n",
    "\n",
    "def score_to_idx(score):\n",
    "    if score < 0.2:\n",
    "        return 0\n",
    "    elif score < 0.4:\n",
    "        return 1\n",
    "    elif score < 0.6:\n",
    "        return 2\n",
    "    elif score < 0.8:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "\n",
    "for instance in islice(sst[\"train\"], 5):\n",
    "    print(\"Raw Instance:\", instance)\n",
    "    print(\"Tokens:\", instance[\"tokens\"].split(\"|\"))\n",
    "    print(\"Label:\", score_to_label(instance[\"label\"]))\n",
    "    print(\"***\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26124e",
   "metadata": {},
   "source": [
    "Below is an implementation of a dataset and classifier:\n",
    "\n",
    "### Read the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "word_to_idx = dict()\n",
    "word_to_idx[PAD_TOKEN] = 0\n",
    "for instance in sst[\"train\"]:\n",
    "    tokens = instance[\"tokens\"].split(\"|\")\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token not in word_to_idx:\n",
    "            word_to_idx[token] = len(word_to_idx)\n",
    "\n",
    "\n",
    "word_to_idx[UNK_TOKEN] = len(word_to_idx)\n",
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f95924",
   "metadata": {},
   "source": [
    "### Load the data into a dataset object\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7399162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: train 8544\n",
      "Example: {'tokens': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'token_idxs': [1, 2, 3, 4, 5, 6, 1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 9, 17, 5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33], 'label': 'positive', 'label_idx': 3}\n",
      "\n",
      "Loaded: validation 1101\n",
      "Example: {'tokens': ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], 'token_idxs': [132, 9, 19, 4074, 266, 93, 4074, 433, 83, 3873, 14, 16582, 33], 'label': 'positive', 'label_idx': 3}\n",
      "\n",
      "Example model input/output\n",
      "([132, 3, 913, 19, 914, 5, 1, 915, 14, 53, 37, 1, 916, 33], 3)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, vocab, partition):\n",
    "        super().__init__()\n",
    "        self.instances = [\n",
    "            {\n",
    "                \"tokens\": instance[\"tokens\"].split(\"|\"),\n",
    "                \"token_idxs\": [\n",
    "                    vocab.get(token.lower(), vocab.get(UNK_TOKEN))\n",
    "                    for token in instance[\"tokens\"].split(\"|\")\n",
    "                ],\n",
    "                \"label\": score_to_label(instance[\"label\"]),\n",
    "                \"label_idx\": score_to_idx(instance[\"label\"]),\n",
    "            }\n",
    "            for instance in sst[partition]\n",
    "        ]\n",
    "\n",
    "        print(\"Loaded:\", partition, len(self.instances))\n",
    "        print(\"Example:\", self.instances[0])\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.instances[idx][\"token_idxs\"], self.instances[idx][\"label_idx\"]\n",
    "\n",
    "\n",
    "train = SSTDataset(word_to_idx, \"train\")\n",
    "validation = SSTDataset(word_to_idx, \"validation\")\n",
    "\n",
    "print(\"Example model input/output\")\n",
    "print(train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59748126",
   "metadata": {},
   "source": [
    "### Make Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03aaf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import pad\n",
    "import torch\n",
    "\n",
    "\n",
    "def pad_and_collate(batch):\n",
    "    Xs = [item[0] for item in batch]\n",
    "    ys = [item[1] for item in batch]\n",
    "\n",
    "    # Find longest input\n",
    "    longest_x = max(len(x) for x in Xs)\n",
    "\n",
    "    # Fill remaining space in all other inputs with a 0 padding token\n",
    "    X_tensors = [pad(torch.LongTensor(x), (0, longest_x - len(x)), value=0) for x in Xs]\n",
    "\n",
    "    # Create one large matrix of Xs\n",
    "    X_tensors = torch.stack(X_tensors)\n",
    "    y_tensors = torch.stack([torch.LongTensor([y]) for y in ys]).squeeze()\n",
    "\n",
    "    return X_tensors, y_tensors\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train, batch_size=32, shuffle=True, collate_fn=pad_and_collate\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation, batch_size=32, shuffle=True, collate_fn=pad_and_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be4bb2",
   "metadata": {},
   "source": [
    "### Define Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79aa553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per class predictions on sample from model\n",
      "tensor([0.0333, 0.0581, 0.0712, 0.1170, 0.0473], grad_fn=<ViewBackward0>)\n",
      "Selected class\n",
      "3\n",
      "Actual class\n",
      "3\n",
      "Cross entropy loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.5582, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "num_embeddings = len(word_to_idx)\n",
    "num_labels = 5\n",
    "embedding_size = 100\n",
    "hidden_size = 50\n",
    "\n",
    "\n",
    "class MLPBaseline(nn.Module):\n",
    "    def __init__(self, num_embeddings, num_labels, embedding_dim, hidden_dim):\n",
    "        super(MLPBaseline, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.nonlinear = torch.tanh\n",
    "        self.cls_layer = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        emb = self.embedding(input_tensor)\n",
    "        summed = self.nonlinear(emb.mean(axis=-2))\n",
    "        hidden = self.nonlinear(self.linear1(summed))\n",
    "        hidden = self.nonlinear(self.linear2(hidden))\n",
    "        hidden = self.nonlinear(self.linear3(hidden))\n",
    "\n",
    "        logits = self.cls_layer(hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "baseline = MLPBaseline(num_embeddings, num_labels, embedding_size, hidden_size)\n",
    "\n",
    "sample_input = torch.LongTensor(train[100][0])\n",
    "logits = baseline(sample_input)\n",
    "\n",
    "print(\"Per class predictions on sample from model\")\n",
    "print(logits)\n",
    "\n",
    "print(\"Selected class\")\n",
    "print(torch.argmax(logits).item())\n",
    "\n",
    "print(\"Actual class\")\n",
    "print(train[100][1])\n",
    "\n",
    "print(\"Cross entropy loss\")\n",
    "ce = nn.CrossEntropyLoss()\n",
    "# Note that the cross entropy loss function expects a batch of inputs, hence using unsqueeze function to make a batch of 1 item.\n",
    "ce(logits.unsqueeze(0), torch.LongTensor([train[100][1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45191ad1",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d70f03f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Average training loss: 0.048203452389115255\n",
      "Average training acc: 0.31062734082397003\n",
      "\n",
      "Average validation loss: 0.04714240744155066\n",
      "Average validation acc: 0.3478655767484105\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "Average training loss: 0.043250743871771\n",
      "Average training acc: 0.39232209737827717\n",
      "\n",
      "Average validation loss: 0.04422772277170263\n",
      "Average validation acc: 0.3832879200726612\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "Average training loss: 0.0364966235552611\n",
      "Average training acc: 0.4874765917602996\n",
      "\n",
      "Average validation loss: 0.04603513699028299\n",
      "Average validation acc: 0.40054495912806537\n",
      "\n",
      "\n",
      "Epoch:  3\n",
      "Average training loss: 0.029745224270313866\n",
      "Average training acc: 0.6036985018726592\n",
      "\n",
      "Average validation loss: 0.05035406324021065\n",
      "Average validation acc: 0.3505903723887375\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Average training loss: 0.022978426644418123\n",
      "Average training acc: 0.7210908239700374\n",
      "\n",
      "Average validation loss: 0.06000980326092102\n",
      "Average validation acc: 0.3478655767484105\n",
      "\n",
      "\n",
      "Epoch:  5\n",
      "Average training loss: 0.01685592338219564\n",
      "Average training acc: 0.8093398876404494\n",
      "\n",
      "Average validation loss: 0.06894231005000809\n",
      "Average validation acc: 0.35603996366939145\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "num_epochs = 6\n",
    "baseline = MLPBaseline(num_embeddings, num_labels, embedding_size, hidden_size)\n",
    "optimizer = Adam(baseline.parameters(), lr=0.005)\n",
    "scheduler = LinearLR(optimizer, total_iters=num_epochs)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_total = 0\n",
    "    train_acc_hits = 0\n",
    "    train_count = 0\n",
    "\n",
    "    valid_loss_total = 0\n",
    "    valid_acc_hits = 0\n",
    "    valid_count = 0\n",
    "\n",
    "    # Sample a batch of sentences Xs and labels ys from the dataset\n",
    "    for Xs, ys in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make a prediction and estimate the loss\n",
    "        prediction = baseline(Xs)\n",
    "        loss = ce(prediction, ys)\n",
    "\n",
    "        # Compute changes for each parameter w.r.t. loss and update model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy and average loss for reporting\n",
    "        train_count += Xs.shape[0]\n",
    "        train_loss_total += loss.item()\n",
    "        train_acc_hits += torch.count_nonzero(\n",
    "            torch.argmax(prediction, dim=-1) == ys\n",
    "        ).item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xs, ys in validation_loader:\n",
    "            prediction = baseline(Xs)\n",
    "            loss = ce(prediction, ys)\n",
    "\n",
    "            valid_count += Xs.shape[0]\n",
    "            valid_loss_total += loss.item()\n",
    "            valid_acc_hits += torch.count_nonzero(\n",
    "                torch.argmax(prediction, dim=-1) == ys\n",
    "            ).item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(\"Average training loss:\", train_loss_total / train_count)\n",
    "    print(\"Average training acc:\", train_acc_hits / train_count)\n",
    "    print()\n",
    "    print(\"Average validation loss:\", valid_loss_total / valid_count)\n",
    "    print(\"Average validation acc:\", valid_acc_hits / valid_count)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f84e9",
   "metadata": {},
   "source": [
    "**Problem 1.1** (1 point) In your own words. Describe the architecture of the provided model. Does the model capture sentence ordering? How is the meaning of the sentence represented?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647049b",
   "metadata": {},
   "source": [
    "The provided model is an MLP model with:\n",
    "\n",
    "- An embedding layer that converts input token indices into dense vectors.\n",
    "- A non-linear transformation (tanh) applied to the mean of the embeddings to get a sentence representation.\n",
    "- Three linear layers followed by $\\tanh$ activations to further process the sentence representation.\n",
    "- A final classification layer to predict sentiment scores.\n",
    "\n",
    "Because it takes the mean of the embeddings, the model does not capture sentence ordering and hence loses word sequence information. The average of a sentence's word embeddings is used to convey its meaning, which means that context between words is not explicitly tracked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2081d",
   "metadata": {},
   "source": [
    "**Problem 1.2** (1 point) Add dropout to the model and select an appropriate value of dropout probability. How does this affect the model accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea7792b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "num_embeddings = len(word_to_idx)\n",
    "num_labels = 5\n",
    "embedding_size = 100\n",
    "hidden_size = 50\n",
    "\n",
    "\n",
    "class MLPBaselineWithDropout(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings, num_labels, embedding_dim, hidden_dim, dropout_p\n",
    "    ):\n",
    "        super(MLPBaselineWithDropout, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.nonlinear = torch.tanh\n",
    "        self.cls_layer = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        emb = self.embedding(input_tensor)\n",
    "        summed = self.nonlinear(emb.mean(axis=-2))\n",
    "\n",
    "        hidden = self.dropout(self.nonlinear(self.linear1(summed)))\n",
    "        hidden = self.dropout(self.nonlinear(self.linear2(hidden)))\n",
    "        hidden = self.dropout(self.nonlinear(self.linear3(hidden)))\n",
    "\n",
    "        logits = self.cls_layer(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4d472f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import fmean\n",
    "\n",
    "\n",
    "dropout_p_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "val_accuracies = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for dropout_p in dropout_p_values:\n",
    "    num_epochs = 6\n",
    "    for i in range(20):\n",
    "        baseline = MLPBaselineWithDropout(\n",
    "            num_embeddings, num_labels, embedding_size, hidden_size, dropout_p\n",
    "        ).to()\n",
    "        optimizer = Adam(baseline.parameters(), lr=0.005)\n",
    "        scheduler = LinearLR(optimizer, total_iters=num_epochs)\n",
    "\n",
    "        tmp_val_accuracies = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss_total = 0\n",
    "            train_acc_hits = 0\n",
    "            train_count = 0\n",
    "\n",
    "            valid_loss_total = 0\n",
    "            valid_acc_hits = 0\n",
    "            valid_count = 0\n",
    "\n",
    "            # Sample a batch of sentences Xs and labels ys from the dataset\n",
    "            for Xs, ys in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Make a prediction and estimate the loss\n",
    "                prediction = baseline(Xs)\n",
    "                loss = criterion(prediction, ys)\n",
    "\n",
    "                # Compute changes for each parameter w.r.t. loss and update model\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute accuracy and average loss for reporting\n",
    "                train_count += Xs.shape[0]\n",
    "                train_loss_total += loss.item()\n",
    "                train_acc_hits += torch.count_nonzero(\n",
    "                    torch.argmax(prediction, dim=-1) == ys\n",
    "                ).item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for Xs, ys in validation_loader:\n",
    "                    prediction = baseline(Xs)\n",
    "                    loss = criterion(prediction, ys)\n",
    "\n",
    "                    valid_count += Xs.shape[0]\n",
    "                    valid_loss_total += loss.item()\n",
    "                    valid_acc_hits += torch.count_nonzero(\n",
    "                        torch.argmax(prediction, dim=-1) == ys\n",
    "                    ).item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            val_accuracy = valid_acc_hits / valid_count\n",
    "\n",
    "        tmp_val_accuracies.append(val_accuracy)\n",
    "\n",
    "    val_accuracies.append(fmean(tmp_val_accuracies))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dropout_p_values, val_accuracies, marker=\"o\")\n",
    "plt.title(\"Dropout Probability vs Validation Accuracy\")\n",
    "plt.xlabel(\"Dropout Probability\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01263021",
   "metadata": {},
   "source": [
    "Dropouts are known to improve model generalisation and prevent overfitting. In this experiment, dropout was added into the model with a range of probabilities [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]. Each probability was tested 20 times to account for variability. The results indicated that a dropout probability of 0.6 yielded the highest validation accuracy. However, it was observed that for dropout values exceeding 0.7, there was a significant decline in validation accuracy, suggesting that a too high dropout rate negatively impacted the model's learning capability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0059709",
   "metadata": {},
   "source": [
    "**Problem 1.3** (3 points) Replace part of a model which aggregates the sentence meaning with an RNN encoder. Retrain the model and report the validation set accuracy.\n",
    "\n",
    "There are several valid ways to use the RNN cell. Chose an appropriate representation and justify your choice in your own words.\n",
    "\n",
    "The documentation for the RNN module is here: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "\n",
    "**Common bug: check the batch_first parameter is set**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d717948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, num_embeddings, num_labels, embedding_dim, hidden_dim):\n",
    "        super(RNNBaseline, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, embedding_dim, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.nonlinear = torch.tanh\n",
    "        self.cls_layer = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        emb = self.embedding(input_tensor)\n",
    "\n",
    "        out, _ = self.rnn(emb)\n",
    "        rnn_out = out[:, -1, :]\n",
    "\n",
    "        hidden = self.nonlinear(self.linear1(rnn_out))\n",
    "        hidden = self.nonlinear(self.linear2(hidden))\n",
    "        hidden = self.nonlinear(self.linear3(hidden))\n",
    "\n",
    "        logits = self.cls_layer(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f73fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Average training loss: 0.04313669292332974\n",
      "Average training acc: 0.26556647940074907\n",
      "\n",
      "Average validation loss: 0.04357356390662891\n",
      "Average validation acc: 0.25340599455040874\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "Average training loss: 0.0427569311377261\n",
      "Average training acc: 0.2646301498127341\n",
      "\n",
      "Average validation loss: 0.04410148478983533\n",
      "Average validation acc: 0.26248864668483196\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "Average training loss: 0.04275690374916859\n",
      "Average training acc: 0.26322565543071164\n",
      "\n",
      "Average validation loss: 0.04356815888598006\n",
      "Average validation acc: 0.25340599455040874\n",
      "\n",
      "\n",
      "Epoch:  3\n",
      "Average training loss: 0.0429028466846166\n",
      "Average training acc: 0.2624063670411985\n",
      "\n",
      "Average validation loss: 0.04417558099658353\n",
      "Average validation acc: 0.26248864668483196\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Average training loss: 0.04295396522971128\n",
      "Average training acc: 0.27153558052434457\n",
      "\n",
      "Average validation loss: 0.04389173631555486\n",
      "Average validation acc: 0.20799273387829245\n",
      "\n",
      "\n",
      "Epoch:  5\n",
      "Average training loss: 0.04297022545214896\n",
      "Average training acc: 0.26205524344569286\n",
      "\n",
      "Average validation loss: 0.04400905568853061\n",
      "Average validation acc: 0.26248864668483196\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "num_epochs = 6\n",
    "\n",
    "num_embeddings = len(word_to_idx)\n",
    "num_labels = 5\n",
    "embedding_size = 100\n",
    "hidden_size = 50\n",
    "\n",
    "rnn_base = RNNBaseline(num_embeddings, num_labels, embedding_size, hidden_size)\n",
    "optimizer = Adam(rnn_base.parameters(), lr=0.005)\n",
    "scheduler = LinearLR(optimizer, total_iters=num_epochs)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_total = 0\n",
    "    train_acc_hits = 0\n",
    "    train_count = 0\n",
    "\n",
    "    valid_loss_total = 0\n",
    "    valid_acc_hits = 0\n",
    "    valid_count = 0\n",
    "\n",
    "    # Sample a batch of sentences Xs and labels ys from the dataset\n",
    "    for Xs, ys in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make a prediction and estimate the loss\n",
    "        prediction = rnn_base(Xs)\n",
    "        loss = ce(prediction, ys)\n",
    "\n",
    "        # Compute changes for each parameter w.r.t. loss and update model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy and average loss for reporting\n",
    "        train_count += Xs.shape[0]\n",
    "        train_loss_total += loss.item()\n",
    "        train_acc_hits += torch.count_nonzero(\n",
    "            torch.argmax(prediction, dim=-1) == ys\n",
    "        ).item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xs, ys in validation_loader:\n",
    "            prediction = rnn_base(Xs)\n",
    "            loss = ce(prediction, ys)\n",
    "\n",
    "            valid_count += Xs.shape[0]\n",
    "            valid_loss_total += loss.item()\n",
    "            valid_acc_hits += torch.count_nonzero(\n",
    "                torch.argmax(prediction, dim=-1) == ys\n",
    "            ).item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(\"Average training loss:\", train_loss_total / train_count)\n",
    "    print(\"Average training acc:\", train_acc_hits / train_count)\n",
    "    print()\n",
    "    print(\"Average validation loss:\", valid_loss_total / valid_count)\n",
    "    print(\"Average validation acc:\", valid_acc_hits / valid_count)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c7681",
   "metadata": {},
   "source": [
    "In this modified model, an RNN layer is introduced to replace the aggregation of sentence meaning done previously by averaging embeddings. The RNN can capture sequential information in the sentence, potentially providing richer sentence representations. The final hidden state of the RNN is used to represent the whole sentence. This representation is then fed into the linear layers to predict the label, as it captures the sequential interactions of the sentence tokens, and should contain the overall contextual information of the entire sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b267e",
   "metadata": {},
   "source": [
    "# Problem 2 - Recurrent Model as an Autoregressive Language Model (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80711804",
   "metadata": {},
   "source": [
    "**Problem 2.1** (1 point) In your own words, describe the RNN neural network architecture. Use equations where appropriate (max 100 words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0807c1",
   "metadata": {},
   "source": [
    "The RNN architecture is designed to process sequential data by maintaining a hidden state $h_t$ that carries information from previous time steps. The core equation for standard RNN is:\n",
    "\n",
    "$$h_t = \\sigma(U^\\top h_{t-1} + V^\\top x_t)$$\n",
    "\n",
    "Here, $x_t$ is the input at time step $t$, $h_{t-1}$ is the hidden state from the previous time step, and $U, V$ are trainable parameters shared between time steps. The hidden state $h_t$ serves as the memory of the network, allowing it to capture information from past inputs in the sequence. The final output at time step $n$ is usually computed as $y_n = \\text{Softmax}(W^\\top h_n)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfa390",
   "metadata": {},
   "source": [
    "**Problem 2.2** (2 points) In your own words. State what the the vanishing gradient problem with RNNs is. How to LSTMs differ from RNNs? How does this difference mitigate the vanishing gradient problem? Use equations where appropriate. (max 200 words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec78bf2",
   "metadata": {},
   "source": [
    "The vanishing gradient problem in RNNs occurs during training when gradients of the loss function w.r.t. the parameters become very small for long sequences, causing the weights not to update effectively, hence learning long-term dependencies becomes challenging. This happens because during backpropagation through time, gradients are multiplied by the weight matrices at each time step, and if the weights are small the gradients shrink exponentially.\n",
    "\n",
    "LSTMs are a type of RNN designed to mitigate the vanishing gradient problem. Unlike standard RNNs, LSTMs have a more complex internal structure involving three gates: an input gate, a forget gate, and an output gate, along with a cell state. The equations for an LSTM are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I_t &= \\sigma(W_{xi}^\\top x_t + W_{hi}^\\top h_{t-1})\\\\\n",
    "F_t &= \\sigma(W_{xf}^\\top x_t + W_{hf}^\\top h_{t-1})\\\\\n",
    "O_t &= \\sigma(W_{xo}^\\top x_t + W_{ho}^\\top h_{t-1})\\\\\n",
    "\\tilde{C}_t &= \\tanh(W_{xc}^\\top x_t + W_{hc}^\\top h_{t-1})\\\\\n",
    "C_t &= F_t * C_{t-1} + I_t * \\tilde{C}_t\\\\\n",
    "h_t &= O_t * \\tanh(C_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here, $*$ denotes elementwise multiplication. The cell state $C_t$ in LSTMs allows the network to maintain long-term dependencies, and the gating mechanisms help in adaptively controlling the flow of information, mitigating the vanishing gradient problem by allowing gradients to flow unchanged through the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ee1d5",
   "metadata": {},
   "source": [
    "**Problem 2.3** (1 point) Consider a sequence of tokens $W=(w_1, w_2,\\ldots,w_T)$ with $T$ tokens. Discuss how teacher forcing can be used to train a language model to predict $w_t | w_1, \\ldots, w_{t-1}$ (max 100 words). Write the maximum likelihood criterion for training the model for this sequence with teacher forcing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb2ee56",
   "metadata": {},
   "source": [
    "In teacher forcing, during training, the true previous tokens $w_1, w_2, \\ldots, w_{t-1}$ are fed as inputs at each time step $t$ to train the model to predict the next token $w_t$. It accelerates training as the model receives accurate historical context.\n",
    "\n",
    "The maximum likelihood criterion for this sequence with teacher forcing can be represented as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(w_t | w_1, w_2, \\ldots, w_{t-1}; \\theta)\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{L}$ is the loss to be minimised, and $\\theta$ represents the parameters of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b02ea0",
   "metadata": {},
   "source": [
    "**Problem 2.4** (5 points) Implement and train an autoregressive LSTM language model using the sentences from the provided SST dataset. Treat each sentence in the dataset independently. Report the training and validation loss for a maximum of 20 epochs. Select appropriate model dimensions.\n",
    "\n",
    "- Hint: it would be beneficial to include special tokens to mark the start and end of sequences.\n",
    "- Hint: it may be beneficial to compute the loss for a single string considering all tokens in a single call to the model rather than perform many independent predictions.\n",
    "\n",
    "Documentation: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f100d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: train 8544\n",
      "Example: {'tokens': ['<SOS>', 'The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.', '<EOS>'], 'token_idxs': [16583, 1, 2, 3, 4, 5, 6, 1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 9, 17, 5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 16584], 'label': 'positive', 'label_idx': 3}\n",
      "\n",
      "Loaded: validation 1101\n",
      "Example: {'tokens': ['<SOS>', 'It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.', '<EOS>'], 'token_idxs': [16583, 132, 9, 19, 4074, 266, 93, 4074, 433, 83, 3873, 14, 16582, 33, 16584], 'label': 'positive', 'label_idx': 3}\n",
      "\n",
      "Example model input/output\n",
      "([16583, 132, 3, 913, 19, 914, 5, 1, 915, 14, 53, 37, 1, 916, 33, 16584], 3)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# Extend vocabulary\n",
    "if \"<SOS>\" not in word_to_idx:\n",
    "    word_to_idx[\"<SOS>\"] = len(word_to_idx)\n",
    "if \"<EOS>\" not in word_to_idx:\n",
    "    word_to_idx[\"<EOS>\"] = len(word_to_idx)\n",
    "\n",
    "\n",
    "class SSTDataset2(Dataset):\n",
    "    def __init__(self, vocab, partition):\n",
    "        super().__init__()\n",
    "        sos_idx = vocab.get(\"<SOS>\")\n",
    "        eos_idx = vocab.get(\"<EOS>\")\n",
    "\n",
    "        self.instances = [\n",
    "            {\n",
    "                \"tokens\": [\"<SOS>\"] + instance[\"tokens\"].split(\"|\") + [\"<EOS>\"],\n",
    "                \"token_idxs\": [sos_idx]\n",
    "                + [\n",
    "                    vocab.get(token.lower(), vocab.get(UNK_TOKEN))\n",
    "                    for token in instance[\"tokens\"].split(\"|\")\n",
    "                ]\n",
    "                + [eos_idx],\n",
    "                \"label\": score_to_label(instance[\"label\"]),\n",
    "                \"label_idx\": score_to_idx(instance[\"label\"]),\n",
    "            }\n",
    "            for instance in sst[partition]\n",
    "        ]\n",
    "\n",
    "        print(\"Loaded:\", partition, len(self.instances))\n",
    "        print(\"Example:\", self.instances[0])\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.instances[idx][\"token_idxs\"], self.instances[idx][\"label_idx\"]\n",
    "\n",
    "\n",
    "train = SSTDataset2(word_to_idx, \"train\")\n",
    "validation = SSTDataset2(word_to_idx, \"validation\")\n",
    "\n",
    "print(\"Example model input/output\")\n",
    "print(train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fa56321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import pad\n",
    "import torch\n",
    "\n",
    "\n",
    "def pad_and_collate(batch):\n",
    "    Xs = [item[0] for item in batch]\n",
    "    ys = [item[1] for item in batch]\n",
    "\n",
    "    # Find longest input\n",
    "    longest_x = max(len(x) for x in Xs)\n",
    "\n",
    "    # Fill remaining space in all other inputs with a 0 padding token\n",
    "    X_tensors = [pad(torch.LongTensor(x), (0, longest_x - len(x)), value=0) for x in Xs]\n",
    "\n",
    "    # Create one large matrix of Xs\n",
    "    X_tensors = torch.stack(X_tensors)\n",
    "    y_tensors = torch.stack([torch.LongTensor([y]) for y in ys]).squeeze()\n",
    "\n",
    "    # print(X_tensors.shape)\n",
    "    # print(y_tensors.shape)\n",
    "    return X_tensors, y_tensors\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train, batch_size=32, shuffle=True, collate_fn=pad_and_collate\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation, batch_size=32, shuffle=True, collate_fn=pad_and_collate\n",
    ")\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a307841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_embeddings = len(word_to_idx)\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=-1)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        out, _ = self.lstm(embed)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a93df152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, num_epochs=20):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.005)\n",
    "    scheduler = LinearLR(optimizer, total_iters=num_epochs)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_total = 0\n",
    "        train_acc_hits = 0\n",
    "        train_count = 0\n",
    "\n",
    "        valid_loss_total = 0\n",
    "        valid_acc_hits = 0\n",
    "        valid_count = 0\n",
    "\n",
    "        # Sample a batch of sentences Xs and labels ys from the dataset\n",
    "        for Xs, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_seq = Xs[:, :-1]\n",
    "            target_seq = Xs[:, 1:]\n",
    "\n",
    "            # Make a prediction and estimate the loss\n",
    "            prediction = model(input_seq)\n",
    "\n",
    "            loss = criterion(\n",
    "                prediction.view(-1, num_embeddings), target_seq.reshape(-1)\n",
    "            )\n",
    "\n",
    "            # Compute changes for each parameter w.r.t. loss and update model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute accuracy and average loss for reporting\n",
    "            train_count += input_seq.numel()\n",
    "            train_loss_total += loss.item()\n",
    "\n",
    "            correct_predictions = torch.argmax(prediction, dim=-1) == target_seq\n",
    "            train_acc_hits += correct_predictions.sum().item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xs, _ in validation_loader:\n",
    "                input_seq = Xs[:, :-1]\n",
    "                target_seq = Xs[:, 1:]\n",
    "\n",
    "                prediction = model(input_seq)\n",
    "\n",
    "                loss = criterion(\n",
    "                    prediction.view(-1, num_embeddings), target_seq.reshape(-1)\n",
    "                )\n",
    "\n",
    "                valid_count += input_seq.numel()\n",
    "                valid_loss_total += loss.item()\n",
    "\n",
    "                correct_predictions = torch.argmax(prediction, dim=-1) == target_seq\n",
    "                valid_acc_hits += correct_predictions.sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return valid_loss_total / valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4737dad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: Embedding Size 50, Hidden Size 50, Num Layers 1\n",
      "Validation loss: 0.002689710155273838\n",
      "\n",
      "Dimensions: Embedding Size 50, Hidden Size 50, Num Layers 2\n",
      "Validation loss: 0.0026488257491308056\n",
      "\n",
      "Dimensions: Embedding Size 50, Hidden Size 100, Num Layers 1\n",
      "Validation loss: 0.0028789147748225583\n",
      "\n",
      "Dimensions: Embedding Size 50, Hidden Size 100, Num Layers 2\n",
      "Validation loss: 0.002751499390654856\n",
      "\n",
      "Dimensions: Embedding Size 100, Hidden Size 50, Num Layers 1\n",
      "Validation loss: 0.0026822667269629243\n",
      "\n",
      "Dimensions: Embedding Size 100, Hidden Size 50, Num Layers 2\n",
      "Validation loss: 0.002711253913529595\n",
      "\n",
      "Dimensions: Embedding Size 100, Hidden Size 100, Num Layers 1\n",
      "Validation loss: 0.0029540488576136298\n",
      "\n",
      "Dimensions: Embedding Size 100, Hidden Size 100, Num Layers 2\n",
      "Validation loss: 0.002917746228386951\n",
      "\n",
      "------------------------------\n",
      "Best Dimensions: Embedding Size 50, Hidden Size 50, Num Layers 2\n"
     ]
    }
   ],
   "source": [
    "embedding_sizes = [50, 100]\n",
    "hidden_sizes = [50, 100]\n",
    "num_layers_list = [1, 2]\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_dims = None\n",
    "best_state_dict = None\n",
    "\n",
    "for embedding_size in embedding_sizes:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for num_layers in num_layers_list:\n",
    "            model = LSTMModel(num_embeddings, embedding_size, hidden_size, num_layers)\n",
    "            curr_dims = (embedding_size, hidden_size, num_layers)\n",
    "\n",
    "            # Train the model and evaluate on validation set\n",
    "            val_loss = train_and_evaluate(model)\n",
    "            print(\n",
    "                \"Dimensions: Embedding Size {}, Hidden Size {}, Num Layers {}\".format(\n",
    "                    *curr_dims\n",
    "                )\n",
    "            )\n",
    "            print(\"Validation loss:\", val_loss)\n",
    "            print()\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_dims = curr_dims\n",
    "                best_state_dict = model.state_dict()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\n",
    "    \"Best Dimensions: Embedding Size {}, Hidden Size {}, Num Layers {}\".format(\n",
    "        *best_dims\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb187de",
   "metadata": {},
   "source": [
    "**Problem 2.4** (1 point) Comparing a randomly initialized model with your trained model, decode a sequence of up to 10 words or until an end of sentence token is predicted. Explore with using greedy decoding and sampling from the distribution of predicted tokens.\n",
    "\n",
    "Show your code and write the sample sentences below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3d6a413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Model: daredevils tool tool tough grumbling grumbling slam-dunk slam-dunk thought thought\n",
      "Trained Model: the movie is a good movie that 's a movie\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def decode(model, start_token_idx, end_token_idx, max_len=10):\n",
    "    model.eval()\n",
    "    sequence = [start_token_idx]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            input_seq = torch.tensor([sequence]).long()\n",
    "            output = model(input_seq)\n",
    "            next_word_idx = output[0, -1, :].argmax().item()\n",
    "            if next_word_idx == end_token_idx:\n",
    "                break\n",
    "            sequence.append(next_word_idx)\n",
    "    return sequence[1:]\n",
    "\n",
    "\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "\n",
    "def decode_sequence(sequence, idx_to_word):\n",
    "    words = [idx_to_word[idx] for idx in sequence]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Randomly Initialized Model\n",
    "random_model = LSTMModel(num_embeddings, *best_dims)\n",
    "random_model.eval()\n",
    "\n",
    "# Trained Model\n",
    "trained_model = LSTMModel(num_embeddings, *best_dims)\n",
    "trained_model.load_state_dict(best_state_dict)\n",
    "trained_model.eval()\n",
    "\n",
    "\n",
    "start_token_idx = word_to_idx[\"<SOS>\"]\n",
    "end_token_idx = word_to_idx[\"<EOS>\"]\n",
    "sequence_random = decode(random_model, start_token_idx, end_token_idx)\n",
    "sequence_trained = decode(trained_model, start_token_idx, end_token_idx)\n",
    "\n",
    "# Decode sequences to sentences\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "print(\"Random Model:\", decode_sequence(sequence_random, idx_to_word))\n",
    "print(\"Trained Model:\", decode_sequence(sequence_trained, idx_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95dfc96",
   "metadata": {},
   "source": [
    "# Problem 3 - Hierarchical Softmax (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971de9e",
   "metadata": {},
   "source": [
    "In Mikolov et al., 2013b (https://arxiv.org/pdf/1310.4546.pdf), Hierarchical softmax estimates the probability of a token as a sequence of binary choices for traversing a tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0ec89",
   "metadata": {},
   "source": [
    "**Problem 3.1** (2 points) In eqn 3 in Mikolov 2013b, the probability for a word is estimated with the Hierarchical Softmax function. The probability of a word is estimated as a the product of binary (left or right) deicsions traversing this tree. Using the definition of the special function in the paper ($\\lbrack\\lbrack x \\rbrack\\rbrack$), show that if $p(w,left) = \\sigma(u_{n(w,j)}^Tv_t)$, then $p(w,right) = \\sigma(- u_{n(w,j)}^Tv_t)$ where $\\sigma$ is sigmoid function.\n",
    "\n",
    "Hint: this requires manipulation of the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c1ebf",
   "metadata": {},
   "source": [
    "The sigmoid function is defined as:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "This implies that:\n",
    "\n",
    "$1 - \\sigma(x) = 1 - \\frac{1}{1 + e^{-x}} = \\frac{e^{-x}}{1 + e^{-x}} = \\frac{1}{e^x + 1} = \\sigma(-x)$\n",
    "\n",
    "We know that $p(w, \\text{right})$ is the complement of $p(w, \\text{left})$. Thus,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(w, \\text{right}) &= 1 - p(w, \\text{left})\\\\\n",
    "\\phantom{p(w, \\text{right})} &= 1 - \\sigma(u_{n(w, j)}^\\top v_t)\\\\\n",
    "\\phantom{p(w, \\text{right})} &= \\sigma(-u_{n(w, j)}^\\top v_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This completes the proof.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d413e61",
   "metadata": {},
   "source": [
    "**Problem 3.2** (3 points) Compare the runtime efficiency of Softmax, Noise Contrastive Estimation, and Hierarchical Softmax for estimating $P(w_t|w_1,\\ldots,w_{t-1})$ (with respect to the number of tokens in the vocabulary). What impact does this have when training the model? Why is this a factor we want to consider? (max 100 words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eca7e",
   "metadata": {},
   "source": [
    "Softmax complexity is $\\mathcal{O}(V)$, where $V$ is the vocabulary size, due to normalisation over all words in the vocabulary.\\\n",
    "Noise Contrastive Estimation complexity is $\\mathcal{O}(k)$, where $k$ is the number of noise samples used.\\\n",
    "Hierarchical Softmax complexity is $\\mathcal{O}(\\log⁡ V)$ by approximating normalisation using a binary tree structure.\n",
    "\n",
    "Reduced computational complexity speeds up training, especially with large vocabularies, enabling the model to handle more data. This is crucial as computational efficiency is often a limiting factor in training large-scale language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e249d0",
   "metadata": {},
   "source": [
    "**Problem 3.3** (5 points) Implement hierarchical softmax for estimating $P(w_t | w_1, \\ldots, w_{t-1})$ in your LSTM LM considering the hidden reprsentation $h_t$ for a token at position $t$. Implement your solution with a balanced binary tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cafeedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSoftmax(nn.Module):\n",
    "    def __init__(self, hidden_dim, vocab_size):\n",
    "        super(HierarchicalSoftmax, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tree_depth = int(np.ceil(np.log2(vocab_size)))\n",
    "        self.total_nodes = 2 ** (self.tree_depth + 1) - 1\n",
    "        self.non_leaf_nodes = 2**self.tree_depth - 1\n",
    "\n",
    "        self.nodes = nn.Parameter(torch.randn(self.non_leaf_nodes, hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _ = x.size()\n",
    "        outputs = torch.zeros(batch_size, self.vocab_size)\n",
    "\n",
    "        for i in range(self.vocab_size):\n",
    "            idx = 0\n",
    "            path_prob = torch.ones(batch_size)\n",
    "            bin_repr = format(\n",
    "                i, \"0\" + str(self.tree_depth) + \"b\"\n",
    "            )  # binary representation of word index\n",
    "            for depth, direction in enumerate(bin_repr):\n",
    "                logit = torch.sigmoid(\n",
    "                    torch.matmul(x, self.nodes[idx].unsqueeze(-1))\n",
    "                ).squeeze()\n",
    "                if direction == \"0\":\n",
    "                    # left\n",
    "                    path_prob *= logit\n",
    "                    idx = 2 * idx + 1\n",
    "                else:\n",
    "                    # right\n",
    "                    path_prob *= 1 - logit\n",
    "                    idx = 2 * idx + 2\n",
    "            outputs[:, i] = path_prob\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "765cb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModelWithHS(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, num_layers):\n",
    "        super(LSTMModelWithHS, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.hierarchical_softmax = HierarchicalSoftmax(hidden_dim, num_embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        out, _ = self.lstm(embed)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.hierarchical_softmax(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "num_embeddings = len(word_to_idx)\n",
    "embedding_size = 100\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = LSTMModelWithHS(num_embeddings, embedding_size, hidden_size, num_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
