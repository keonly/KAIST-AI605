{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc0963f",
   "metadata": {},
   "source": [
    "# 2023 Fall AI605 Assignment 3: Information Retrieval\n",
    "\n",
    "## Rubric\n",
    "\n",
    "### Deadline\n",
    "\n",
    "The deadline for this assignment is: Friday 17th November 2023 (Week 12) 11:59pm\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit your assignment via [KLMS](https://klms.kaist.ac.kr). You must submit the Jupyter Notebook file (.ipynb) with all code and model outputs.\n",
    "\n",
    "Use in-line LaTeX for mathematical expressions.\n",
    "\n",
    "### Collaboration\n",
    "\n",
    "This assignment is an individual assingnment. It is **not** a group assignment so make sure your answer and code are your own.\n",
    "\n",
    "### Grading\n",
    "\n",
    "The total number of marks avaiable is 25 points. There is a bonus question for +5 marks. If you score >25 points, these marks will be distributed to your other assignments.\n",
    "\n",
    "### Environment\n",
    "\n",
    "The use of a GPU is recommended for problem 1.5 and problem 2, limit computation time to approximately 2 hours with a GPU for each task. The suggested environment for this is Python 3.9. Run the following cell to set up the environment.\n",
    "\n",
    "### Data\n",
    "\n",
    "Both questions will use a simplified subset of the FEVER dataset. The FEVER dataset is introduced here: https://aclanthology.org/N18-1074/ . For both questions you **MUST USE THE VERSION THAT IS UPLOADED TO KLMS.**\n",
    "\n",
    "### Libraries\n",
    "\n",
    "The following libraries should be used for the project. You should not need any other libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc53cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch tqdm datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b63e1",
   "metadata": {},
   "source": [
    "# Problem 1 - Passage Reranking with BERT (12 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17ec8f",
   "metadata": {},
   "source": [
    "This question is based on the following paper: \"Passage Reranking with BERT\" from Rodrigo Nogueira and Kyunghyun Cho: https://arxiv.org/pdf/1901.04085.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd16dc1",
   "metadata": {},
   "source": [
    "**Problem 1.1** (1 point) Describe how the BERT model is utilized for the task of passage re-ranking in the paper. What makes BERT an effective model for this task, as opposed to traditional information retrieval models such as TF-IDF and BM25?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c65e9",
   "metadata": {},
   "source": [
    "The query is truncated to a maximum of 64 tokens and then input as Sentence A. The text of the passage is similarly truncated to ensure that the combined length of the query, the passage, and any separator tokens does not exceed 512 tokens, and this is input as Sentence B. The `[CLS]` vector is then used as the input for a single-layer neural network, which is used to calculate the probability of the passage's relevance. This probability is computed for each passage independently. Finally, passages are organised into a list based on their respective probabilities, thereby ranking them according to their relevance. The training begins with a pre-trained BERT Large model, which is then specifically adapted to the re-ranking task through the application of cross-entropy loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{j \\in J_\\text{pos}} \\log (s_j) - \\sum_{j \\in J_\\text{neg}} \\log (1 - s_j)\n",
    "$$\n",
    "\n",
    "where $J_\\text{pos}$ is the set of indexes of the relevant passages, and $J_\\text{neg}$ is the set of indexes of non-relevant passages in top-1,000 documents retrieved with BM25.\n",
    "\n",
    "BERT is an effective model for the task of re-ranking passages because the pre-training on a vast corpus of data gives BERT a broad understanding of language and semantics. This pre-trained knowledge is then fine-tuned for the specific task of passage re-ranking, making it more robust and efficient. It is mentioned in the paper that the pretrained models used in the work required few training examples from the end task to achieve a good performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80297f55",
   "metadata": {},
   "source": [
    "**Problem 1.2** (2 points) The paper employs BERT in a cross-encoder setup for re-ranking. Explain the cross-encoder architecture and how it differs from a bi-encoder architecture (such as DPR) in terms of input format and model inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6234df1",
   "metadata": {},
   "source": [
    "- **Cross-Encoder Architecture**\n",
    "\n",
    "  The input to a cross-encoder architecture model is a single sequence consisting of both texts (the query and the passage) concatenated together. This sequence typically begins with a special start token (`[CLS]` in BERT), followed by the first text (query), and the second text (passage), separated by a separator token (`[SEP]` in BERT). The cross-encoder processes this concatenated sequence as one. The output of the model is usually derived from the embedding of the start token by feeding the embedding to a classifier FC layer. This architecture can capture the interaction between sentences very effectively. However, it is computationally expensive because the encoding for each pair of sentences must be recomputed, even though they share one sentence.\n",
    "\n",
    "- **Bi-Encoder Architecture**\n",
    "\n",
    "  In a bi-encoder architecture model, the query and passage are processed separately. There are two encoder networks in a bi-encoder architecture, and each text is passed through its corresponding encoder, producing independent embeddings for each text. These separate embeddings are then compared to determine the relevance of the passage to the query. This comparison can be as simple as computing the dot product or cosine similarity between the two embeddings. This architecture can capture only a limited amount of interaction between sentences. However, since sentences can be computed and indexed beforehand, it is computationally lightweight. If a sentence is shared between different pairs, the precomputed value can be used instead of recomputing the encoding for every pair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea2cb3",
   "metadata": {},
   "source": [
    "**Problem 1.3** (1 point) Analyze the computational efficiency of using BERT for passage re-ranking, as discussed in the paper. How does the complexity of the BERT model affect its scalability and usage in a real-world setting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30886b20",
   "metadata": {},
   "source": [
    "BERT is a highly complex model with a large number of parameters. Processing each query-passage pair involves a significant amount of computation, particularly because BERT uses a cross-encoder architecture where the query and passage are combined into a single sequence for processing. Thus, using BERT directly for passage re-ranking would not be very optimal because it cannot be scaled to take, for instance, millions of documents. As a cross-encoder architecture requires to process each query-passage _pair_ individually for relevance score computation, the overall efficiency of the architecture would be extremely low, making the architecture difficult to scale to a large number of sentence pairs.\n",
    "\n",
    "The paper uses BERT as part of a multi-stage method to overcome these difficulties. In the first stage, a more computationally efficient method such as BM25 is used for initial retrieval. This stage narrows down the candidate set from potentially millions of documents to a smaller, more manageable number for the second stage. In the second stage, BERT is used for re-ranking this smaller set of candidates. Although the initial retrieval stage is quick and effective, it may not be as precise or sophisticated in identifying the linkages between the queries and passages. The second stage (re-ranking with BERT) adds this understanding and improves accuracy but is applied only to a limited set of candidates due to its computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cea9a1",
   "metadata": {},
   "source": [
    "**Problem 1.4** (2 points) Examine the training objective given by equation 1. Describe the role and importance of positive and negative passages in this training objective. How do these contribute to the model's learning process in the context of passage re-ranking?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff369da",
   "metadata": {},
   "source": [
    "The training objective is used to train the model to distinguish between relevant and non-relevant passages for a given query. (In my opinion, the objective has some similar aspects to metric learning, where the objective is to increase the distance between embeddings of different classes, while decreasing the distance of the embeddings of the same class, thereby making the class embeddings more 'distinguishable.' Similarly, in this context, the model aims to more effectively distinguish between positive and negative passage instances, i.e., identifying which passages are relevant and which are not.)\n",
    "\n",
    "Positive passages are examples that are relevant to the query. The model learns from these examples by understanding the patterns, keywords, and contextual signs that signify relevance.\n",
    "\n",
    "On the other hand, negative passages are not relevant to the query. The model learns from these examples what irrelevance looks like, and how to differentiate between relevant and non-relevant content. Without negative examples, the model might overfit to the positive examples and fail to generalise well to unseen queries and passages. This is critical in passage (re-)ranking scenarios where distinguishing subtle differences in relevance is very important.\n",
    "\n",
    "The choice of negative passages is important because the properties of the classifier will differ based on the sampling strategy used. One might consider randomly sampling negative passages. The two sequences (the query and the negative passage) in this case would probably be logically and semantically distinct by nature, making the training process quite easy. In this scenario, the decision boundary the model should learn would be simple, but its power would be limited. A different approach would be to deliberately select negative texts for sampling, making the query and the negative passage semantically similar yet irrelevant. (One example professor mentioned in the class is the query being about Louis XIV, and the negative passage being about Louis XV.) The decision boundary in this case is more challenging, and the model would need to pay closer attention to the subtle differences between the positive and negative passages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eae788",
   "metadata": {},
   "source": [
    "**Problem 1.5** (6 points) Using the provided data, implement the passage reranker with BERT paper by fine-tuning the `bert-base-cased` model in HuggingFace and report the MRR@10 on the test set. At test time, you should report MRR using the position of the positive passages considering all the passages (concatenating positive and negative passage lists).\n",
    "\n",
    "The a simplified version of the FEVER dataset is uploaded to KLMS. The data is provided in JSON lines format where each line in the file is a JSON document. Each element in the training set contains: the query, a list of positive pages, a list of negative passages. A description of the data is provided here: https://aclanthology.org/N18-1074/\n",
    "\n",
    "Rubric:\n",
    "\n",
    "- 1 point - tokenization and data preparation\n",
    "- 2 points - loss function\n",
    "- 1 point - training loop and appropriate hyperparameters\n",
    "- 1 point - performing inference for a set of passages\n",
    "- 1 point - successful convergence and reporting of MRR@10\n",
    "\n",
    "If you have limited resources, it is acceptable to use a subset of the training data. But still aim to use 1-2 hours of GPU compute for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b2605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109810it [00:15, 7291.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "defaultdict(<class 'int'>, {10: 100069, 9: 4376, 12: 392, 11: 2607, 3: 38, 8: 1095, 7: 446, 0: 17, 6: 258, 14: 53, 15: 29, 4: 84, 16: 20, 5: 121, 13: 145, 17: 8, 19: 2, 2: 24, 1: 15, 18: 3, 20: 5, 21: 1, 22: 1, 25: 1})\n",
      "0.911292232037155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6666it [00:00, 7369.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "defaultdict(<class 'int'>, {10: 105942, 9: 4688, 12: 408, 11: 2869, 3: 39, 8: 1190, 7: 498, 0: 22, 6: 275, 14: 55, 15: 29, 4: 90, 16: 21, 5: 134, 13: 148, 17: 8, 19: 2, 2: 27, 1: 19, 18: 3, 20: 5, 21: 1, 22: 1, 25: 1, 23: 1})\n",
      "0.9095607678835125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6666it [00:00, 7924.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "defaultdict(<class 'int'>, {10: 111944, 9: 4912, 12: 434, 11: 3130, 3: 43, 8: 1252, 7: 540, 0: 22, 6: 290, 14: 61, 15: 31, 4: 93, 16: 22, 5: 145, 13: 153, 17: 9, 19: 2, 2: 28, 1: 19, 18: 3, 20: 5, 21: 1, 22: 1, 25: 1, 23: 1})\n",
      "0.9090643322343311\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "num_passages = defaultdict(int)\n",
    "filepaths = [\n",
    "    \"./fever_data/fever_retrieval_train.jsonl\",\n",
    "    \"./fever_data/fever_retrieval_dev.jsonl\",\n",
    "    \"./fever_data/fever_retrieval_test.jsonl\",\n",
    "]\n",
    "names = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "for name, path in zip(names, filepaths):\n",
    "    with open(path) as f:\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            inst = json.loads(line)\n",
    "\n",
    "            positive_passages = [\n",
    "                list(d.values())[0] for d in inst[\"positive_passages\"][0]\n",
    "            ]\n",
    "            negative_passages = [\n",
    "                list(d.values())[0] for d in inst[\"negative_passages\"][0]\n",
    "            ]\n",
    "\n",
    "            num_passages[len(positive_passages) + len(negative_passages)] += 1\n",
    "\n",
    "    print(name)\n",
    "    print(num_passages)\n",
    "    print(\n",
    "        num_passages[max(num_passages, key=num_passages.get)]\n",
    "        / sum(num_passages.values())\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36158713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save(\n",
    "    file, tokenizer, save_file_name, query_max_len=64, total_max_len=512\n",
    "):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    num_queries = len(lines)\n",
    "    num_passages_per_query = 10\n",
    "\n",
    "    all_passage_encodings = torch.zeros(\n",
    "        (num_queries, num_passages_per_query, total_max_len), dtype=torch.int32\n",
    "    )\n",
    "    all_labels = torch.zeros((num_queries, num_passages_per_query), dtype=torch.int32)\n",
    "\n",
    "    for i, line in enumerate(tqdm(lines)):\n",
    "        inst = json.loads(line)\n",
    "        query = inst[\"query\"]\n",
    "        passages = [list(d.values())[0] for d in inst[\"positive_passages\"][0]] + [\n",
    "            list(d.values())[0] for d in inst[\"negative_passages\"][0]\n",
    "        ]\n",
    "\n",
    "        if len(passages) != num_passages_per_query:\n",
    "            continue\n",
    "\n",
    "        labels = [1] * len(inst[\"positive_passages\"][0]) + [0] * len(\n",
    "            inst[\"negative_passages\"][0]\n",
    "        )\n",
    "\n",
    "        query_encoding = tokenizer.encode(\n",
    "            query,\n",
    "            add_special_tokens=True,\n",
    "            max_length=query_max_len,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        query_len = len(query_encoding)\n",
    "\n",
    "        for j, passage in enumerate(passages):\n",
    "            passage_encoding = tokenizer.encode(\n",
    "                passage,\n",
    "                add_special_tokens=True,\n",
    "                max_length=total_max_len - query_len + 1,  # Remove [CLS]\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "            combined_encoding = query_encoding + passage_encoding[1:]\n",
    "            combined_encoding = torch.tensor(combined_encoding)\n",
    "            assert len(combined_encoding) == total_max_len\n",
    "\n",
    "            all_passage_encodings[i, j, : len(combined_encoding)] = combined_encoding\n",
    "            all_labels[i, j] = labels[j]\n",
    "\n",
    "    torch.save((all_passage_encodings, all_labels), save_file_name)\n",
    "\n",
    "    del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e8ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832a9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109810/109810 [24:12<00:00, 75.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [01:22<00:00, 80.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [01:26<00:00, 76.76it/s]\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\n",
    "    \"./fever_data/fever_retrieval_train.jsonl\",\n",
    "    \"./fever_data/fever_retrieval_dev.jsonl\",\n",
    "    \"./fever_data/fever_retrieval_test.jsonl\",\n",
    "]\n",
    "names = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "for name, path in zip(names, filepaths):\n",
    "    save_file_name = f\"./fever_data/fever_encoded_{name}.pt\"\n",
    "    print(f\"Preprocessing {name} data...\")\n",
    "    preprocess_and_save(path, tokenizer, save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ace89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassageRerankingDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.encodings, self.labels = torch.load(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.encodings[idx]  # shape: [10, 512]\n",
    "\n",
    "        attention_mask = (input_ids != 0).int()  # shape: [10, 512]\n",
    "        token_type_ids = torch.zeros_like(input_ids)  # shape: [10, 512]\n",
    "        labels = self.labels[idx]  # shape: [10]\n",
    "\n",
    "        sep_indices = (input_ids == 102).nonzero(as_tuple=True)\n",
    "\n",
    "        for i, sep_idx in enumerate(sep_indices[1][::2]):\n",
    "            token_type_ids[i, sep_idx + 1 :] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c971f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PassageRerankingDataset(\"./fever_data/fever_encoded_train.pt\")\n",
    "test_dataset = PassageRerankingDataset(\"./fever_data/fever_encoded_test.pt\")\n",
    "val_dataset = PassageRerankingDataset(\"./fever_data/fever_encoded_dev.pt\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7044f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassageRerankingModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.linear = torch.nn.Linear(768, 1)\n",
    "        torch.nn.init.kaiming_normal_(self.linear.weight)\n",
    "\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        labels = kwargs.pop(\"labels\", None)\n",
    "\n",
    "        outputs = self.bert(**kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.linear(pooled_output)\n",
    "\n",
    "        loss = self.criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad3f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "model = PassageRerankingModel()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=3e-6, betas=(0.9, 0.999), weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"ai605-assignment3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65383d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "max_training_time = 5 * 60 * 60  # 5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2fbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(val_loader):\n",
    "            batch = {k: v.squeeze().to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_val_loss += outputs[\"loss\"]\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3f0c89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 0, Training Loss: 7.3323\n",
      "Epoch 1/5, Iteration 0, Validation Loss: 7.2717\n",
      "Epoch 1/5, Iteration 100, Training Loss: 1.2706\n",
      "Epoch 1/5, Iteration 200, Training Loss: 1.9903\n",
      "Epoch 1/5, Iteration 300, Training Loss: 1.9184\n",
      "Epoch 1/5, Iteration 400, Training Loss: 0.2270\n",
      "Epoch 1/5, Iteration 500, Training Loss: 0.4033\n",
      "Epoch 1/5, Iteration 600, Training Loss: 0.6345\n",
      "Epoch 1/5, Iteration 700, Training Loss: 0.3796\n",
      "Epoch 1/5, Iteration 800, Training Loss: 0.4182\n",
      "Epoch 1/5, Iteration 900, Training Loss: 1.0051\n",
      "Epoch 1/5, Iteration 1000, Training Loss: 2.0050\n",
      "Epoch 1/5, Iteration 1000, Validation Loss: 0.9970\n",
      "Epoch 1/5, Iteration 1100, Training Loss: 0.7463\n",
      "Epoch 1/5, Iteration 1200, Training Loss: 0.7014\n",
      "Epoch 1/5, Iteration 1300, Training Loss: 3.5718\n",
      "Epoch 1/5, Iteration 1400, Training Loss: 0.5062\n",
      "Epoch 1/5, Iteration 1500, Training Loss: 0.3172\n",
      "Epoch 1/5, Iteration 1600, Training Loss: 1.9304\n",
      "Epoch 1/5, Iteration 1700, Training Loss: 0.0591\n",
      "Epoch 1/5, Iteration 1800, Training Loss: 0.6257\n",
      "Epoch 1/5, Iteration 1900, Training Loss: 0.5512\n",
      "Epoch 1/5, Iteration 2000, Training Loss: 0.9937\n",
      "Epoch 1/5, Iteration 2000, Validation Loss: 0.8706\n",
      "Epoch 1/5, Iteration 2100, Training Loss: 1.0743\n",
      "Epoch 1/5, Iteration 2200, Training Loss: 1.3909\n",
      "Epoch 1/5, Iteration 2300, Training Loss: 0.4074\n",
      "Epoch 1/5, Iteration 2400, Training Loss: 0.4247\n",
      "Epoch 1/5, Iteration 2500, Training Loss: 0.0089\n",
      "Epoch 1/5, Iteration 2600, Training Loss: 0.6401\n",
      "Epoch 1/5, Iteration 2700, Training Loss: 0.6638\n",
      "Epoch 1/5, Iteration 2800, Training Loss: 0.4703\n",
      "Epoch 1/5, Iteration 2900, Training Loss: 0.4801\n",
      "Epoch 1/5, Iteration 3000, Training Loss: 0.3612\n",
      "Epoch 1/5, Iteration 3000, Validation Loss: 0.8343\n",
      "Epoch 1/5, Iteration 3100, Training Loss: 0.0052\n",
      "Epoch 1/5, Iteration 3200, Training Loss: 0.4737\n",
      "Epoch 1/5, Iteration 3300, Training Loss: 2.2330\n",
      "Epoch 1/5, Iteration 3400, Training Loss: 0.0045\n",
      "Epoch 1/5, Iteration 3500, Training Loss: 2.6758\n",
      "Epoch 1/5, Iteration 3600, Training Loss: 1.5408\n",
      "Epoch 1/5, Iteration 3700, Training Loss: 0.9890\n",
      "Epoch 1/5, Iteration 3800, Training Loss: 0.6581\n",
      "Epoch 1/5, Iteration 3900, Training Loss: 0.5840\n",
      "Epoch 1/5, Iteration 4000, Training Loss: 2.3414\n",
      "Epoch 1/5, Iteration 4000, Validation Loss: 0.8358\n",
      "Epoch 1/5, Iteration 4100, Training Loss: 0.7099\n",
      "Epoch 1/5, Iteration 4200, Training Loss: 0.0037\n",
      "Epoch 1/5, Iteration 4300, Training Loss: 1.3875\n",
      "Epoch 1/5, Iteration 4400, Training Loss: 0.0024\n",
      "Epoch 1/5, Iteration 4500, Training Loss: 0.3071\n",
      "Epoch 1/5, Iteration 4600, Training Loss: 0.2229\n",
      "Epoch 1/5, Iteration 4700, Training Loss: 0.4165\n",
      "Epoch 1/5, Iteration 4800, Training Loss: 0.0218\n",
      "Epoch 1/5, Iteration 4900, Training Loss: 1.1199\n",
      "Epoch 1/5, Iteration 5000, Training Loss: 0.7816\n",
      "Epoch 1/5, Iteration 5000, Validation Loss: 0.7900\n",
      "Epoch 1/5, Iteration 5100, Training Loss: 1.6032\n",
      "Epoch 1/5, Iteration 5200, Training Loss: 0.4347\n",
      "Epoch 1/5, Iteration 5300, Training Loss: 0.3262\n",
      "Epoch 1/5, Iteration 5400, Training Loss: 0.2315\n",
      "Epoch 1/5, Iteration 5500, Training Loss: 0.3650\n",
      "Epoch 1/5, Iteration 5600, Training Loss: 0.9367\n",
      "Epoch 1/5, Iteration 5700, Training Loss: 0.2390\n",
      "Epoch 1/5, Iteration 5800, Training Loss: 1.2240\n",
      "Epoch 1/5, Iteration 5900, Training Loss: 0.3694\n",
      "Epoch 1/5, Iteration 6000, Training Loss: 0.0048\n",
      "Epoch 1/5, Iteration 6000, Validation Loss: 0.8370\n",
      "Epoch 1/5, Iteration 6100, Training Loss: 0.3642\n",
      "Epoch 1/5, Iteration 6200, Training Loss: 0.3762\n",
      "Epoch 1/5, Iteration 6300, Training Loss: 0.0061\n",
      "Epoch 1/5, Iteration 6400, Training Loss: 0.7868\n",
      "Epoch 1/5, Iteration 6500, Training Loss: 0.1863\n",
      "Epoch 1/5, Iteration 6600, Training Loss: 1.1959\n",
      "Epoch 1/5, Iteration 6700, Training Loss: 0.7981\n",
      "Epoch 1/5, Iteration 6800, Training Loss: 0.0134\n",
      "Epoch 1/5, Iteration 6900, Training Loss: 1.1622\n",
      "Epoch 1/5, Iteration 7000, Training Loss: 0.1517\n",
      "Epoch 1/5, Iteration 7000, Validation Loss: 0.8501\n",
      "Epoch 1/5, Iteration 7100, Training Loss: 0.2360\n",
      "Epoch 1/5, Iteration 7200, Training Loss: 0.3472\n",
      "Epoch 1/5, Iteration 7300, Training Loss: 0.5132\n",
      "Epoch 1/5, Iteration 7400, Training Loss: 1.3244\n",
      "Epoch 1/5, Iteration 7500, Training Loss: 3.2746\n",
      "Epoch 1/5, Iteration 7600, Training Loss: 0.5009\n",
      "Epoch 1/5, Iteration 7700, Training Loss: 0.5039\n",
      "Epoch 1/5, Iteration 7800, Training Loss: 5.7014\n",
      "Epoch 1/5, Iteration 7900, Training Loss: 1.8386\n",
      "Epoch 1/5, Iteration 8000, Training Loss: 0.1525\n",
      "Epoch 1/5, Iteration 8000, Validation Loss: 0.8545\n",
      "Epoch 1/5, Iteration 8100, Training Loss: 0.9536\n",
      "Epoch 1/5, Iteration 8200, Training Loss: 0.1661\n",
      "Epoch 1/5, Iteration 8300, Training Loss: 0.1435\n",
      "Epoch 1/5, Iteration 8400, Training Loss: 8.8686\n",
      "Epoch 1/5, Iteration 8500, Training Loss: 0.2491\n",
      "Epoch 1/5, Iteration 8600, Training Loss: 0.1088\n",
      "Epoch 1/5, Iteration 8700, Training Loss: 0.1835\n",
      "Epoch 1/5, Iteration 8800, Training Loss: 0.1351\n",
      "Epoch 1/5, Iteration 8900, Training Loss: 0.0018\n",
      "Epoch 1/5, Iteration 9000, Training Loss: 5.4641\n",
      "Epoch 1/5, Iteration 9000, Validation Loss: 0.7519\n",
      "Epoch 1/5, Iteration 9100, Training Loss: 3.0913\n",
      "Epoch 1/5, Iteration 9200, Training Loss: 0.1074\n",
      "Epoch 1/5, Iteration 9300, Training Loss: 0.2103\n",
      "Epoch 1/5, Iteration 9400, Training Loss: 1.0902\n",
      "Epoch 1/5, Iteration 9500, Training Loss: 0.0371\n",
      "Epoch 1/5, Iteration 9600, Training Loss: 0.3306\n",
      "Epoch 1/5, Iteration 9700, Training Loss: 0.0038\n",
      "Epoch 1/5, Iteration 9800, Training Loss: 0.0166\n",
      "Epoch 1/5, Iteration 9900, Training Loss: 1.9742\n",
      "Epoch 1/5, Iteration 10000, Training Loss: 0.1066\n",
      "Epoch 1/5, Iteration 10000, Validation Loss: 0.8672\n",
      "Epoch 1/5, Iteration 10100, Training Loss: 0.0033\n",
      "Epoch 1/5, Iteration 10200, Training Loss: 0.1431\n",
      "Epoch 1/5, Iteration 10300, Training Loss: 0.1853\n",
      "Epoch 1/5, Iteration 10400, Training Loss: 0.1686\n",
      "Epoch 1/5, Iteration 10500, Training Loss: 2.1285\n",
      "Epoch 1/5, Iteration 10600, Training Loss: 3.3506\n",
      "Epoch 1/5, Iteration 10700, Training Loss: 0.3062\n",
      "Epoch 1/5, Iteration 10800, Training Loss: 0.5466\n",
      "Epoch 1/5, Iteration 10900, Training Loss: 0.1767\n",
      "Epoch 1/5, Iteration 11000, Training Loss: 0.0924\n",
      "Epoch 1/5, Iteration 11000, Validation Loss: 0.8513\n",
      "Epoch 1/5, Iteration 11100, Training Loss: 0.2613\n",
      "Epoch 1/5, Iteration 11200, Training Loss: 1.7358\n",
      "Epoch 1/5, Iteration 11300, Training Loss: 0.5636\n",
      "Epoch 1/5, Iteration 11400, Training Loss: 0.1069\n",
      "Epoch 1/5, Iteration 11500, Training Loss: 2.3386\n",
      "Epoch 1/5, Iteration 11600, Training Loss: 0.2179\n",
      "Epoch 1/5, Iteration 11700, Training Loss: 0.6279\n",
      "Epoch 1/5, Iteration 11800, Training Loss: 2.8622\n",
      "Epoch 1/5, Iteration 11900, Training Loss: 0.0744\n",
      "Epoch 1/5, Iteration 12000, Training Loss: 0.5178\n",
      "Epoch 1/5, Iteration 12000, Validation Loss: 1.0669\n",
      "Epoch 1/5, Iteration 12100, Training Loss: 0.0977\n",
      "Epoch 1/5, Iteration 12200, Training Loss: 0.0303\n",
      "Epoch 1/5, Iteration 12300, Training Loss: 0.4603\n",
      "Epoch 1/5, Iteration 12400, Training Loss: 0.0958\n",
      "Epoch 1/5, Iteration 12500, Training Loss: 0.3885\n",
      "Epoch 1/5, Iteration 12600, Training Loss: 0.5382\n",
      "Epoch 1/5, Iteration 12700, Training Loss: 1.0744\n",
      "Epoch 1/5, Iteration 12800, Training Loss: 0.3079\n",
      "Epoch 1/5, Iteration 12900, Training Loss: 0.0930\n",
      "Epoch 1/5, Iteration 13000, Training Loss: 0.0012\n",
      "Epoch 1/5, Iteration 13000, Validation Loss: 0.9197\n",
      "Epoch 1/5, Iteration 13100, Training Loss: 2.3217\n",
      "Epoch 1/5, Iteration 13200, Training Loss: 0.0610\n",
      "Epoch 1/5, Iteration 13300, Training Loss: 1.4073\n",
      "Epoch 1/5, Iteration 13400, Training Loss: 0.3947\n",
      "Epoch 1/5, Iteration 13500, Training Loss: 0.0011\n",
      "Epoch 1/5, Iteration 13600, Training Loss: 0.4513\n",
      "Epoch 1/5, Iteration 13700, Training Loss: 0.2876\n",
      "Epoch 1/5, Iteration 13800, Training Loss: 0.9145\n",
      "Epoch 1/5, Iteration 13900, Training Loss: 0.3502\n",
      "Epoch 1/5, Iteration 14000, Training Loss: 0.6495\n",
      "Epoch 1/5, Iteration 14000, Validation Loss: 0.8829\n",
      "Epoch 1/5, Iteration 14100, Training Loss: 0.1834\n",
      "Epoch 1/5, Iteration 14200, Training Loss: 0.2414\n",
      "Epoch 1/5, Iteration 14300, Training Loss: 0.0012\n",
      "Epoch 1/5, Iteration 14400, Training Loss: 0.9092\n",
      "Epoch 1/5, Iteration 14500, Training Loss: 0.0009\n",
      "Epoch 1/5, Iteration 14600, Training Loss: 0.2204\n",
      "Epoch 1/5, Iteration 14700, Training Loss: 0.1459\n",
      "Epoch 1/5, Iteration 14800, Training Loss: 0.8671\n",
      "Epoch 1/5, Iteration 14900, Training Loss: 0.1662\n",
      "Epoch 1/5, Iteration 15000, Training Loss: 0.5142\n",
      "Epoch 1/5, Iteration 15000, Validation Loss: 0.8885\n",
      "Epoch 1/5, Iteration 15100, Training Loss: 0.0943\n",
      "Epoch 1/5, Iteration 15200, Training Loss: 0.3994\n",
      "Epoch 1/5, Iteration 15300, Training Loss: 0.2623\n",
      "Epoch 1/5, Iteration 15400, Training Loss: 0.0380\n",
      "Epoch 1/5, Iteration 15500, Training Loss: 0.0135\n",
      "Epoch 1/5, Iteration 15600, Training Loss: 0.0012\n",
      "Epoch 1/5, Iteration 15700, Training Loss: 0.0492\n",
      "Epoch 1/5, Iteration 15800, Training Loss: 0.0008\n",
      "Epoch 1/5, Iteration 15900, Training Loss: 0.8460\n",
      "Epoch 1/5, Iteration 16000, Training Loss: 0.2514\n",
      "Epoch 1/5, Iteration 16000, Validation Loss: 0.8467\n",
      "Epoch 1/5, Iteration 16100, Training Loss: 0.0368\n",
      "Epoch 1/5, Iteration 16200, Training Loss: 0.0722\n",
      "Epoch 1/5, Iteration 16300, Training Loss: 0.0470\n",
      "Epoch 1/5, Iteration 16400, Training Loss: 0.3206\n",
      "Epoch 1/5, Iteration 16500, Training Loss: 0.1973\n",
      "Epoch 1/5, Iteration 16600, Training Loss: 0.0672\n",
      "Epoch 1/5, Iteration 16700, Training Loss: 0.1116\n",
      "Epoch 1/5, Iteration 16800, Training Loss: 0.3878\n",
      "Epoch 1/5, Iteration 16900, Training Loss: 1.1160\n",
      "Epoch 1/5, Iteration 17000, Training Loss: 0.0019\n",
      "Epoch 1/5, Iteration 17000, Validation Loss: 0.9361\n",
      "Epoch 1/5, Iteration 17100, Training Loss: 3.0687\n",
      "Epoch 1/5, Iteration 17200, Training Loss: 0.0251\n",
      "Epoch 1/5, Iteration 17300, Training Loss: 0.1403\n",
      "Epoch 1/5, Iteration 17400, Training Loss: 0.3708\n",
      "Epoch 1/5, Iteration 17500, Training Loss: 0.4467\n",
      "Epoch 1/5, Iteration 17600, Training Loss: 0.6181\n",
      "Epoch 1/5, Iteration 17700, Training Loss: 0.4285\n",
      "Epoch 1/5, Iteration 17800, Training Loss: 0.2702\n",
      "Epoch 1/5, Iteration 17900, Training Loss: 0.5631\n",
      "Epoch 1/5, Iteration 18000, Training Loss: 3.4860\n",
      "Epoch 1/5, Iteration 18000, Validation Loss: 0.8478\n",
      "Epoch 1/5, Iteration 18100, Training Loss: 1.5413\n",
      "Epoch 1/5, Iteration 18200, Training Loss: 0.2695\n",
      "Epoch 1/5, Iteration 18300, Training Loss: 0.5017\n",
      "Epoch 1/5, Iteration 18400, Training Loss: 0.2027\n",
      "Epoch 1/5, Iteration 18500, Training Loss: 0.0473\n",
      "Epoch 1/5, Iteration 18600, Training Loss: 0.5676\n",
      "Epoch 1/5, Iteration 18700, Training Loss: 3.2217\n",
      "Epoch 1/5, Iteration 18800, Training Loss: 0.0748\n",
      "Epoch 1/5, Iteration 18900, Training Loss: 1.5427\n",
      "Epoch 1/5, Iteration 19000, Training Loss: 0.0010\n",
      "Epoch 1/5, Iteration 19000, Validation Loss: 0.9664\n",
      "Epoch 1/5, Iteration 19100, Training Loss: 0.2160\n",
      "Epoch 1/5, Iteration 19200, Training Loss: 1.2657\n",
      "Epoch 1/5, Iteration 19300, Training Loss: 0.1977\n",
      "Epoch 1/5, Iteration 19400, Training Loss: 0.2621\n",
      "Epoch 1/5, Iteration 19500, Training Loss: 0.4831\n",
      "Epoch 1/5, Iteration 19600, Training Loss: 0.0896\n",
      "Epoch 1/5, Iteration 19700, Training Loss: 0.3097\n",
      "Epoch 1/5, Iteration 19800, Training Loss: 0.0097\n",
      "Epoch 1/5, Iteration 19900, Training Loss: 1.2726\n",
      "Epoch 1/5, Iteration 20000, Training Loss: 0.0546\n",
      "Epoch 1/5, Iteration 20000, Validation Loss: 0.8681\n",
      "Epoch 1/5, Iteration 20100, Training Loss: 0.0812\n",
      "Epoch 1/5, Iteration 20200, Training Loss: 0.0014\n",
      "Epoch 1/5, Iteration 20300, Training Loss: 0.2305\n",
      "Epoch 1/5, Iteration 20400, Training Loss: 0.0929\n",
      "Epoch 1/5, Iteration 20500, Training Loss: 0.0670\n",
      "Epoch 1/5, Iteration 20600, Training Loss: 1.5743\n",
      "Epoch 1/5, Iteration 20700, Training Loss: 0.1638\n",
      "Epoch 1/5, Iteration 20800, Training Loss: 0.2848\n",
      "Epoch 1/5, Iteration 20900, Training Loss: 1.5959\n",
      "Epoch 1/5, Iteration 21000, Training Loss: 0.0025\n",
      "Epoch 1/5, Iteration 21000, Validation Loss: 0.8583\n",
      "Epoch 1/5, Iteration 21100, Training Loss: 1.1517\n",
      "Epoch 1/5, Iteration 21200, Training Loss: 0.0013\n",
      "Epoch 1/5, Iteration 21300, Training Loss: 0.8020\n",
      "Epoch 1/5, Iteration 21400, Training Loss: 0.1035\n",
      "Epoch 1/5, Iteration 21500, Training Loss: 2.9621\n",
      "Epoch 1/5, Iteration 21600, Training Loss: 0.0830\n",
      "Epoch 1/5, Iteration 21700, Training Loss: 0.1282\n",
      "Epoch 1/5, Iteration 21800, Training Loss: 1.0310\n",
      "Epoch 1/5, Iteration 21900, Training Loss: 0.0698\n",
      "Epoch 1/5, Iteration 22000, Training Loss: 0.1511\n",
      "Epoch 1/5, Iteration 22000, Validation Loss: 0.9137\n",
      "Epoch 1/5, Iteration 22100, Training Loss: 0.0860\n",
      "Epoch 1/5, Iteration 22200, Training Loss: 0.1476\n",
      "Epoch 1/5, Iteration 22300, Training Loss: 0.2989\n",
      "Epoch 1/5, Iteration 22400, Training Loss: 0.5110\n",
      "Epoch 1/5, Iteration 22500, Training Loss: 0.0017\n",
      "Epoch 1/5, Iteration 22600, Training Loss: 0.0279\n",
      "Epoch 1/5, Iteration 22700, Training Loss: 0.0729\n",
      "Epoch 1/5, Iteration 22800, Training Loss: 0.0025\n",
      "Epoch 1/5, Iteration 22900, Training Loss: 0.0016\n",
      "Epoch 1/5, Iteration 23000, Training Loss: 0.5880\n",
      "Epoch 1/5, Iteration 23000, Validation Loss: 0.8597\n",
      "Epoch 1/5, Iteration 23100, Training Loss: 0.0024\n",
      "Epoch 1/5, Iteration 23200, Training Loss: 5.3059\n",
      "Epoch 1/5, Iteration 23300, Training Loss: 0.1496\n",
      "Epoch 1/5, Iteration 23400, Training Loss: 0.0947\n",
      "Epoch 1/5, Iteration 23500, Training Loss: 0.0012\n",
      "Epoch 1/5, Iteration 23600, Training Loss: 0.1261\n",
      "Epoch 1/5, Iteration 23700, Training Loss: 0.0686\n",
      "Epoch 1/5, Iteration 23800, Training Loss: 0.1384\n",
      "Epoch 1/5, Iteration 23900, Training Loss: 0.0602\n",
      "Epoch 1/5, Iteration 24000, Training Loss: 0.0524\n",
      "Epoch 1/5, Iteration 24000, Validation Loss: 0.8906\n",
      "Reached maximum training time. Stopping training.\n",
      "Reached maximum training time. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for train_iter, batch in enumerate(train_loader):\n",
    "        current_time = time.time()\n",
    "        if (current_time - start_time) > max_training_time:\n",
    "            print(\"Reached maximum training time. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        batch = {k: v.squeeze().to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if train_iter % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs}, Iteration {train_iter}, Training Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "        if train_iter % 1000 == 0:\n",
    "            val_loss = evaluate_model(model, val_loader, device)\n",
    "            wandb.log({\"Validation Loss\": val_loss})\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs}, Iteration {train_iter}, Validation Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss,\n",
    "        },\n",
    "        f\"./bert_reranking_epoch{epoch}.pth\",\n",
    "    )\n",
    "\n",
    "    if (time.time() - start_time) > max_training_time:\n",
    "        print(\"Reached maximum training time. Stopping training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba931a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr_at_10(model, dataset):\n",
    "    mrr = 0.0\n",
    "    num_queries = 0\n",
    "\n",
    "    model.eval()\n",
    "    for _, data in enumerate(tqdm(dataset)):\n",
    "        num_queries += 1\n",
    "\n",
    "        kwargs = {k: v.squeeze().to(device) for k, v in data.items()}\n",
    "        labels = kwargs[\"labels\"].tolist()\n",
    "\n",
    "        outputs = model(**kwargs)\n",
    "        probs = torch.sigmoid(outputs[\"logits\"])\n",
    "        query_scores = probs.squeeze().tolist()\n",
    "\n",
    "        ranked_pairs = sorted(\n",
    "            zip(query_scores, labels), key=lambda x: x[0], reverse=True\n",
    "        )\n",
    "        ranked_labels = [label for _, label in ranked_pairs]\n",
    "        ranks = [i + 1 for i, label in enumerate(ranked_labels) if label == 1]\n",
    "\n",
    "        if ranks:\n",
    "            mrr += sum([1.0 / rank for rank in ranks]) / len(ranks)\n",
    "\n",
    "    return mrr / num_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f4ad960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [18:39<00:00,  5.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10: 0.5269481412426958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mrr_at_10 = calculate_mrr_at_10(model, test_loader)\n",
    "print(f\"MRR@10: {mrr_at_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be261c6d",
   "metadata": {},
   "source": [
    "# Problem 2 - Dense Passage Retriever (13 points)\n",
    "\n",
    "This question is based on the following paper: \"Dense Passage Retrieval for Open-Domain Question Answering\" from Vladimir Karpukhin et al., https://aclanthology.org/2020.emnlp-main.550/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2afe4d",
   "metadata": {},
   "source": [
    "**Problem 2.1** (1 point) Discuss how the dense passage retrieval approach differs from traditional sparse retrieval methods. Compare the size of the vectors and their sparsity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0713e88b",
   "metadata": {},
   "source": [
    "- **Sparse Retrieval**\n",
    "\n",
    "  Sparse retrieval methods such as BM25 represent text as high dimensional sparse vectors. Each dimension corresponds to a specific word in the vocabulary. The vast majority of elements in these vectors are zero, indicating the absence of most words in each specific document or query. The vectors are typically very large (often tens of thousands to millions of dimensions) due to the size of the vocabulary.\n",
    "\n",
    "- **Dense Retrieval**\n",
    "\n",
    "  DPR maps text (questions and passages) to lower dimensional continuous space, where each dimension does not correspond to specific individual terms. Instead, these vectors capture semantic meanings and contextual information. Unlike sparse vectors, dense vectors have non-zero values in most dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b96498",
   "metadata": {},
   "source": [
    "**Problem 2.2** (1 point) State the loss function for training the DPR model. The loss function makes use of negative samples, discuss the relationship between the differnt types of negative examples and the overall quality of the retriever model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d965b",
   "metadata": {},
   "source": [
    "The loss function used for training the DPR model is the negative log likelihood of the positive passage:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(q_i, p_i^+, p_{i, 1}^-, \\cdots, p_{i,n}^-) = -\\log \\frac{\\exp(\\text{sim}(q_i, p_i^+))}{\\exp(\\text{sim}(q_i, p_i^+)) + \\sum_{j=1}^n \\exp(\\text{sim}(q_i, p_{i, j}^-))}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\text{sim}(q, p) = E_Q(q)^\\top E_P(p)\n",
    "$$\n",
    "\n",
    "is the dot product similarity between the question and the passage encodings.\n",
    "\n",
    "This is also called the '[multi-class N-pair loss](https://papers.nips.cc/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html)', and is a commonly used loss function in terms of deep metric learning. The loss function is optimised to distinguish the positive passage from the negatives. It is formulated to increase the similarity between the embeddings of the question and its relevant passage while decreasing the similarity with the irrelevant passages.\n",
    "\n",
    "The paper discusses three types of negative examples.\n",
    "\n",
    "- **Random Negatives**\n",
    "\n",
    "  Any random passage from the corpus. As discussed in problem 1.4, in this case the decision boundary the model should learn would be simple, but its power would be limited.\n",
    "\n",
    "- **BM25 Negatives**\n",
    "\n",
    "  Top passages returned by BM25 that don't contain the answer but match most question tokens. They are useful as they represent a more challenging set of negatives. However, they require additional computational overhead, making it difficult to scale the model for a large dataset.\n",
    "\n",
    "- **Gold Negatives**\n",
    "\n",
    "  Positive passages paired with different questions appearing in the training set. They provide a challenging set of negatives while also being computationally efficient.\n",
    "\n",
    "The best model proposed in the paper uses gold passages from the same mini-batch and one BM25 negative passage. It is mentioned that re-using gold passages from the same batch as negatives can make computation efficient while still achieving great performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b88db-8a69-42ee-b15b-bf04a32d72df",
   "metadata": {},
   "source": [
    "**Problem 2.3** (1 points) Explain the process of inferring the most relevant passages to a given question using the inner product. What are the computational advantages of this approach during inference? (compare against Problem 1). Include considerations for scalability when dealing with a large corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d0cdc",
   "metadata": {},
   "source": [
    "DPR uses two independent BERT networks as encoders; the representation at the `[CLS]` token is taken as the output. A dense encoder $E_P(\\cdot)$ maps any text passage to a 768-dimensional real-valued vectors and builds an index. A different encoder, $E_Q(\\cdot)$ maps the input question to a 768-dimensional vector. Then, the inner product (or dot product) is calculated between the vector representation of the question and each passage vector as a metric of similarity. based on these similarity scores, the top $k$-ranked passages are retrieved as the most relevant passages to a given question.\n",
    "\n",
    "The DPR approach allows for pre-computing and indexing the passage vectors. During inference, only the question needs to be encoded, and its vector is then compared with the pre-computed passage encoding vectors. This is computationally more efficient than encoding both the question and each passage at run-time. For a large corpus, the model can scale to handle very large datasets efficiently, as the expensive computation of passage embeddings is done only once and reused for every query.\n",
    "\n",
    "In contrast, a cross-encoder architecture as used in Problem 1 computes the relevance of a passage to a question by jointly encoding them, and is computationally more intensive during inference. This is because for each query, it requires the joint encoding of the question with each passage in the corpus, and this needs to be repeated for every new question. This approach is time-consuming and scales poorly with the size of the corpus, making it impractical for large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf963a2-7a8d-43db-9f8f-9dcce29f0720",
   "metadata": {},
   "source": [
    "**Problem 2.4** (2 points) Using your answer to question 2.3, show how the model can be used for inference to select the top k passages for a given query. Show this using the huggingface `bert-base-cased` model and a sample of 5 instances from the FEVER dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d7f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a982908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfbf5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "model_testing = BertModel.from_pretrained(\"bert-base-cased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9e32a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\n",
      "Top 10 passages:\n",
      "440.1233: [CLS] CBS - LRB - an initialism of the network's former name, the Columbia Broadcasting System - RRB - is an American English language commercial broadcast television network that is a flagship property of CBS Corporation. The company is headquartered at the CBS Building in New York City with major production facilities and operations in New York City - LRB - at the CBS Broadcast Center - RRB - and Los Angeles - LRB - at CBS Television City and the CBS Studio Center - RRB -. CBS is sometimes referred to as the ` ` Eye Network'', in reference to the company's iconic logo, in use since 1951. It has also been called the ` ` Tiffany Network'', alluding to the perceived high quality of CBS programming during the tenure of William S. Paley. It can also refer to some of CBS's first demonstrations of color television, which were held in a former Tiffany & Co. building in New York City in 1950. The network has its origins in United Independent Broadcasters Inc., a collection of 16 radio stations that was purchased by Paley in 1928 and renamed the Columbia Broadcasting System. Under Paley's guidance, CBS would first become one of the largest radio networks in the United States, and eventually one of the Big Three American broadcast television networks. In 1974, CBS dropped its former full name and became known simply as CBS, Inc.. The Westinghouse Electric Corporation acquired the network in 1995, renamed its corporate entity to the current CBS Broadcasting, Inc. in 1997, and eventually adopted the name of the company it had acquired to become CBS Corporation. In 2000, CBS came under the control of Viacom, which was formed as a spin - off of CBS in 1971. In late 2005, Viacom split itself into two separate companies, and re - established CBS Corporation - - through the spin - off of its broadcast television, radio and select cable television and non - broadcasting assets - - with the CBS television network at its core. CBS Corporation is controlled by Sumner Redstone through National Amusements, which also controls the current Viacom. CBS continues to operate the CBS Radio network, which now mainly provides news and features content for its portfolio of owned - and - operated radio stations in large and mid - sized markets, and affiliated radio stations in various other markets. The television network has more than 240 owned - and - operated and affiliated television stations throughout the United States. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "426.3903: [CLS] The National Broadcasting Company - LRB - NBC - RRB - is an American English language commercial broadcast television network that is a flagship property of NBCUniversal, a subsidiary of Comcast. The network is headquartered in the Comcast Building - LRB - formerly known as the GE Building - RRB - at Rockefeller Center in New York City, with additional major offices near Los Angeles - LRB - at Universal City Plaza - RRB -, Chicago - LRB - at the NBC Tower - RRB - and soon in Philadelphia at Comcast Innovation and Technology Center. The network is part of the Big Three television networks. NBC is sometimes referred to as the ` ` Peacock Network'', in reference to its stylized peacock logo, which was originally created in 1956 for its then - new color broadcasts and became the network's official emblem in 1979. Founded in 1926 by the Radio Corporation of America - LRB - RCA - RRB -, NBC is the oldest major broadcast network in the United States. In 1986, control of NBC passed to General Electric - LRB - GE - RRB - - - which previously owned RCA and NBC until 1930, when it was forced to sell the companies as a result of antitrust charges - - through its $ 6. 4 billion purchase of RCA. Following the acquisition by GE - LRB - which later liquidated RCA - RRB -, Bob Wright served as chief executive officer of NBC, remaining in that position until his retirement in 2007, when he was succeeded by Jeff Zucker. In 2003, French media company Vivendi merged its entertainment assets with GE, forming NBC Universal. Comcast purchased a controlling interest in the company in 2011, and acquired General Electric's remaining stake in 2013. Following the Comcast merger, Zucker left NBC Universal and was replaced as CEO by Comcast executive Steve Burke. NBC has thirteen owned - and - operated stations and nearly 200 affiliates throughout the United States and its territories, some of which are also available in Canada via pay - television providers or in border areas over - the - air ; NBC also maintains brand licensing agreements for international channels in South Korea and Germany. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "408.8924: [CLS] MTV - LRB - originally an initialism of Music Television - RRB - is an American cable and satellite television channel owned by Viacom Media Networks - LRB - a division of Viacom - RRB - and headquartered in New York City. Launched on August 1, 1981, the channel originally aired music videos as guided by television personalities known as ` ` video jockeys'' - LRB - VJs - RRB -. In its early years, MTV's main target demographic was young adults, but today it is primarily towards teenagers, particularly high school and college students. MTV has toned down its music video programming significantly in recent years, and its programming now consists mainly of original reality, comedy and drama programming and some off - network syndicated programs and films, with limited music video programming in off - peak time periods. It has received criticism towards this change of focus, both by certain segments of its audience and musicians. MTV's influence on its audience, including issues involving censorship and social activism, has also been a subject of debate for several years. In recent years, MTV had struggled with the secular decline of music - related cable media. Its ratings had been said to be failing systematically, as younger viewers increasingly shift towards digital media, with yearly ratings drops as high as 29 % ; thus there was doubt of the lasting relevance of MTV towards young audiences. In April 2016, MTV announced it would start to return to its original music roots with the reintroduction of the classic MTV series MTV Unplugged. It was also reported that the series MTV Cribs would be making a return on Snapchat, with fourteen original music - related shows speculated to be in production. MTV has spawned numerous sister channels in the US and affiliated channels internationally, some of which have gone independent. As of July 2015, approximately 92, 188, 000 US households - LRB - 79. 2 % of households with television - RRB - have received MTV. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "401.8576: [CLS] The Fox Broadcasting Company - LRB - often shortened to Fox and stylized as FOX - RRB - is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox. The network is headquartered at the 20th Century Fox studio lot on Pico Boulevard in the Century City section of Los Angeles, with additional major offices and production facilities at the Fox Television Center in nearby West Los Angeles and the Fox Broadcasting Center in the Yorkville neighborhood of Manhattan, New York City. It is the third largest major television network in the world based on total revenues, assets, and international coverage. Launched on October 9, 1986 as a competitor to the Big Three television networks - LRB - ABC, NBC and CBS - RRB -, Fox went on to become the most successful attempt at a fourth television network. It was the highest - rated broadcast network in the 18 - - 49 demographic from 2004 to 2012, and earned the position as the most - watched American television network in total viewership during the 2007 - - 08 season. Fox and its affiliated companies operate many entertainment channels in international markets, although these do not necessarily air the same programming as the U. S. network. Most viewers in Canada have access to at least one U. S. - based Fox affiliate, either over - the - air or through a pay television provider, although Fox's National Football League telecasts and most of its prime time programming are subject to simultaneous substitution regulations for cable and satellite providers imposed by the Canadian Radio - television and Telecommunications Commission - LRB - CRTC - RRB - to protect rights held by domestically based networks. The network is named after sister company 20th Century Fox, and indirectly for producer William Fox, who founded one of the movie studio's predecessors, Fox Film. Fox is a member of the North American Broadcasters Association and the National Association of Broadcasters. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "396.7506: [CLS] Nick Jr. is an American digital cable and satellite television channel that is run by the Nickelodeon Group, a unit of the Viacom Media Networks division of Viacom, the channel's ultimate owner. The channel, which is aimed at preschoolers aged 6 and under, features a mix of originally - produced programming, and series previously and concurrently aired on the ` ` Nick : The Smart Place to Play'' block, and its previous iterations, on Nickelodeon. Due to the Nickelodeon block, Nick Jr. is sometimes disclaimed on air as ` ` the Nick Jr. channel'' to avert confusion, especially times of day where both Nickelodeon and Nick Jr. are both carrying preschool programming. The channel was originally known as Noggin from its February 2, 1999 launch until September 28, 2009. Sister channel The N was relaunched as TeenNick at the same time as Noggin's relaunch as Nick Jr. ; as with TeenNick, Nick Jr.'s name was taken from a former program block on parent channel Nickelodeon, which aired weekday mornings from 1988 to 2009 under the Nick Jr. name ; and still survives today on Nickelodeon as a block known in promotions also known as ` ` Nick Jr.'' since 2014 - LRB - which regularly airs from 8 : 30 a. m. to 2 : 00 p. m. ET ; 7 : 00 to 10 : 00 a. m. ET during the summer months or on designated school break periods and major national holidays - RRB -, which has traditional commercial breaks and no common continuity between each series. As of February 2015, Nick Jr. is available to approximately 75. 4 million pay television households - LRB - 64. 8 % of households with television - RRB - in the United States. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "339.7144: [CLS] Nikolaj Coster - Waldau - LRB - - LSB - [UNK] [UNK] [UNK] - RSB - ; born 27 July 1970 - RRB - is a Danish actor, producer and screenwriter. He graduated from Danish National School of Theatre in Copenhagen in 1993. Coster - Waldau's breakthrough performance in Denmark was his role in the film Nightwatch - LRB - 1994 - RRB -. Since then he has appeared in numerous films in his native Scandinavia and Europe in general, including Headhunters - LRB - 2011 - RRB - and A Thousand Times Good Night - LRB - 2013 - RRB -. In the United States, his debut film role was in the war film Black Hawk Down - LRB - 2001 - RRB -, playing Medal of Honor recipient Gary Gordon. He then played Detective John Amsterdam in the short - lived Fox television series New Amsterdam - LRB - 2008 - RRB -, as well as appearing as Frank Pike in the 2009 Fox television film Virtuality, originally intended as a pilot. He became widely known to a broad audience for his current role as Ser Jaime Lannister, in the HBO series Game of Thrones. In 2017, he became one of the highest paid actors on television and earned # 2 million per episode of Game of Thrones. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "225.0340: [CLS] Discovery Channel - LRB - formerly The Discovery Channel from 1985 to 1995, and often referred to as simply Discovery - RRB - is an American basic cable and satellite television channel - LRB - which is also delivered via IPTV, terrestrial television and internet television in other parts of the world - RRB - that is the flagship television property of Discovery Communications, a publicly traded company run by CEO David Zaslav., Discovery Channel is the third most widely distributed cable channel in the United States, behind TBS and The Weather Channel ; it is available in 409 million households worldwide, through its U. S. flagship channel and its various owned or licensed television channels internationally. It initially provided documentary television programming focused primarily on popular science, technology, and history, but in recent years has expanded into reality television and pseudo - scientific entertainment. Programming on the flagship Discovery Channel in the U. S. is primarily focused on reality television series, such as speculative investigation - LRB - with shows such as MythBusters, Unsolved History, and Best Evidence - RRB -, automobiles, and occupations - LRB - such as Dirty Jobs and Deadliest Catch - RRB - ; it also features documentaries specifically aimed at families and younger audiences. A popular annual feature on the channel is Shark Week, which airs on Discovery during the summer months. Despite its popularity and success, the program has garnered criticism, especially from the scientific community, for being scientifically inaccurate., Discovery Channel is available to approximately 96, 589, 000 pay television households - LRB - 83 % of households with television - RRB - in the United States. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "55.5591: [CLS] Nickelodeon - LRB - often shortened to and known in the United States as Nick - RRB - is an American basic cable and satellite television network launched on December 1, 1977, and is owned by Viacom through Viacom Media Networks and based in New York City. It is primarily aimed at children and adolescents aged 8 - - 17, while its weekday morning edutainment programs are targeted at younger children aged 0 - - 7., Nickelodeon is available to approximately 93. 7 million pay television households - LRB - 80. 5 % of households with at least one television set - RRB - in the United States. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "17.1223: [CLS] Nickelodeon is a 1976 comedy film directed by Peter Bogdanovich, and stars Ryan O'Neal, Burt Reynolds and Tatum O'Neal. According to Bogdanovich, the film was based on true stories told to him by silent movie directors Allan Dwan and Raoul Walsh. It was entered into the 27th Berlin International Film Festival. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "10.2769: [CLS] Murdoch Mysteries is a Canadian television drama series aired on both City and CBC Television, titled The Artful Detective on the Ovation cable TV network, featuring Yannick Bisson as William Murdoch, a police detective working in Toronto, Ontario, around the turn of the twentieth century. The television series is based on characters from the novel series by Maureen Jennings. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Query: Roman Atwood is a content creator.\n",
      "Top 10 passages:\n",
      "409.7761: [CLS] YouTube is an American video - sharing website headquartered in San Bruno, California. The service was created by three former PayPal employees - - Chad Hurley, Steve Chen, and Jawed Karim - - in February 2005. Google bought the site in November 2006 for US $ 1. 65 billion ; YouTube now operates as one of Google's subsidiaries. The site allows users to upload, view, rate, share, add to favorites, report and comment on videos, subscribe to other users, and it makes use of WebM, H. 264 / MPEG - 4 AVC, and Adobe Flash Video technology to display a wide variety of user - generated and corporate media videos. Available content includes video clips, TV show clips, music videos, short and documentary films, audio recordings, movie trailers and other content such as video blogging, short original videos, and educational videos. Most of the content on YouTube has been uploaded by individuals, but media corporations including CBS, the BBC, Vevo, and Hulu offer some of their material via YouTube as part of the YouTube partnership program. Unregistered users can only watch videos on the site, while registered users are permitted to upload an unlimited number of videos and add comments to videos. Videos deemed potentially offensive are available only to registered users affirming themselves to be at least 18 years old. YouTube earns advertising revenue from Google AdSense, a program which targets ads according to site content and audience. The vast majority of its videos are free to view, but there are exceptions, including subscription - based premium channels, film rentals, as well as YouTube Red, a subscription service offering ad - free access to the website and access to exclusive content made in partnership with existing users., there are more than 400 hours of content uploaded to YouTube each minute, and one billion hours of content is watched on YouTube every day., the website is ranked as the second most popular site in the world by Alexa Internet, a web traffic analysis company. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "346.6709: [CLS] The British Broadcasting Corporation - LRB - BBC - RRB - is a British public service broadcaster headquartered at Broadcasting House in London. The BBC is the world's oldest national broadcasting organisation and the largest broadcaster in the world by number of employees. It employs over 20, 950 staff in total, 16, 672 of whom are in public sector broadcasting. The total number of staff is 35, 402 when part - time, flexible, and fixed contract staff are included. The BBC is established under a Royal Charter and operates under its Agreement with the Secretary of State for Culture, Media and Sport. Its work is funded principally by an annual television licence fee which is charged to all British households, companies, and organisations using any type of equipment to receive or record live television broadcasts and ` iPlayer'catch - up since 1 September 2016. The fee is set by the British Government, agreed by Parliament, and used to fund the BBC's radio, TV, and online services covering the nations and regions of the UK. Since 1 April 2014, it has also funded the BBC World Service - LRB - launched in 1932 as the BBC Empire Service - RRB -, which broadcasts in 28 languages and provides comprehensive TV, radio, and online services in Arabic and Persian. Around a quarter of BBC revenues come from its commercial arm BBC Worldwide Ltd, which sells BBC programmes and services internationally and also distributes the BBC's international 24 - hour English - language news services BBC World News, and from BBC. com, provided by BBC Global News Ltd.. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "298.0198: [CLS] Romance films or romance movies are romantic love stories recorded in visual media for broadcast in theaters and on TV that focus on passion, emotion, and the affectionate romantic involvement of the main characters and the journey that their genuinely strong, true and pure romantic love takes them through dating, courtship or marriage. Romance films make the romantic love story or the search for strong and pure love and romance the main plot focus. Occasionally, romance lovers face obstacles such as finances, physical illness, various forms of discrimination, psychological restraints or family that threaten to break their union of love. As in all quite strong, deep, and close romantic relationships, tensions of day - to - day life, temptations - LRB - of infidelity - RRB -, and differences in compatibility enter into the plots of romantic films. Romantic films often explore the essential themes of love at first sight, young with older love, unrequited romantic love, obsessive love, sentimental love, spiritual love, forbidden love / romance, platonic love, sexual and passionate love, sacrificial love, explosive and destructive love, and tragic love. Romantic films serve as great escapes and fantasies for viewers, especially if the two people finally overcome their difficulties, declare their love, and experience life ` ` happily ever after'', implied by a reunion and final kiss. In romantic television series, the development of such romantic relationships may play out over many episodes, and different characters may become intertwined in different romantic arcs. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "280.9033: [CLS] Creative consultant is a credit that has - particularly in the past - been given to screenwriters who have consulted on a movie screenplay. Those given this credit in the television field work closely with an executive producer and head writer / showrunner. They are involved in the writing process - LRB - proposing and editing story outlines / scripts - RRB -. Sometimes they are given the credit of executive consultant, story consultant or script consultant. ` ` Creative consultant'' is not listed by the Writers Guild of America as one of its standard credits to be given to writers in television and film. The WGA discourages the use of credits not included on their list and requires that a waiver be obtained in order to credit someone as a creative consultant in television or film. Tom Mankiewicz's credit as Creative Consultant for the 1978 film Superman appeared after the writers'credits, leading to a dispute which Mankiewicz ultimately won. In 1993, the producers of Ace Ventura : Pet Detective were fined by the WGA for giving writer Steve Oedekerk a creative consultant credit. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "277.4678: [CLS] A creative director is a position often found within the graphic design, film, music, video game, fashion, advertising, media, or entertainment industries, but may be useful in other creative organizations such as web development and software development firms as well. A creative director is a vital role in all of the arts and entertainment industries. In another sense, they can be seen as another element in any product development process. The creative director may also assume the roles of an art director, copywriter, or lead designer. The responsibilities of a creative director include leading the communication design, interactive design, and concept forward in any work assigned. For example, this responsibility is often seen in industries related to advertisement. The creative director is known to guide a team of employees with skills and experience related to graphic design, fine arts, motion graphics, and other creative industry fields. Some example works can include visual layout, brainstorming, and copy writing. Before one assumes the role of a creative director, one must have a preset of experience beforehand. Like anyone else, these types of artists start up from the very beginning in fields that can relate to motion graphics, advertisement in television, and / or book - LRB - or magazine - RRB - publishing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "229.5286: [CLS] Roman Bernard Atwood - LRB - born May 28, 1983 - RRB - is an American YouTube personality, comedian, vlogger and pranker. He is best known for his vlogs, where he posts updates about his life on a daily basis. His vlogging channel, ` ` RomanAtwoodVlogs'', has a total of 3. 3 billion views and 11. 9 million subscribers. He also has another YouTube channel called ` ` RomanAtwood'', where he posts pranks. His prank videos have gained over 1. 4 billion views and 10. 3 million subscribers. Both of these channels are in the top 100 most subscribed on YouTube, and he became the second YouTuber after Germán Garmendia to receive two Diamond Play Buttons for his two channels. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "122.2570: [CLS] A media proprietor, media mogul or media tycoon refers to a successful entrepreneur or businessperson who controls, through personal ownership or via a dominant position in any media related company or enterprise, media consumed by a large number of individuals. Those with significant control, ownership, and influence of a large company in the mass media may also be called a tycoon, baron, or business magnate. Social media creators and founders can also be considered media moguls, as such channels deliver media to a large consumer base. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "106.0025: [CLS] The British Academy of Film and Television Arts - LRB - BAFTA - RRB - is an independent charity that supports, develops and promotes the art forms of the moving image - - film, television and game in the United Kingdom. In addition to its annual awards ceremonies, BAFTA has an international, year - round programme of learning events and initiatives offering access to talent through workshops, masterclasses, scholarships, lectures and mentoring schemes in the UK and the USA. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "91.8960: [CLS] A presenter is a person who introduces or hosts television programs - LRB - or segments thereof such as an infomercial advertiser - RRB -. Nowadays, it is common for minor celebrities in other fields to take on this role, but some people have made their name solely within the field of presenting, particularly within children's television series, to become television personalities. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "85.0988: [CLS] The British Academy Television Awards also known as the BAFTA Television Awards are presented in an annual award show hosted by the British Academy of Film and Television Arts - LRB - BAFTA - RRB -. They have been awarded annually since 1955. It is the British equivalent to the Emmy Awards in the United States. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Query: History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.\n",
      "Top 10 passages:\n",
      "386.1466: [CLS] Music is an art form and cultural activity whose medium is sound organized in time. The common elements of music are pitch - LRB - which governs melody and harmony - RRB -, rhythm - LRB - and its associated concepts tempo, meter, and articulation - RRB -, dynamics - LRB - loudness and softness - RRB -, and the sonic qualities of timbre and texture - LRB - which are sometimes termed the ` ` color'' of a musical sound - RRB -. Different styles or types of music may emphasize, de - emphasize or omit some of these elements. Music is performed with a vast range of instruments and vocal techniques ranging from singing to rapping ; there are solely instrumental pieces, solely vocal pieces - LRB - such as songs without instrumental accompaniment - RRB - and pieces that combine singing and instruments. The word derives from Greek μουσική # Ancient Greek - LRB - mousike ; ` ` art of the Muses'' - RRB -. In its most general form, the activities describing music as an art form or cultural activity include the creation of works of music - LRB - songs, tunes, symphonies, and so on - RRB -, the criticism of music, the study of the history of music, and the aesthetic examination of music. Ancient Greek and Indian philosophers defined music as tones ordered horizontally as melodies and vertically as harmonies. Common sayings such as ` ` the harmony of the spheres'' and ` ` it is music to my ears'' point to the notion that music is often ordered and pleasant to listen to. However, 20th - century composer John Cage thought that any sound can be music, saying, for example, ` ` There is no noise, only sound.'' The creation, performance, significance, and even the definition of music vary according to culture and social context. Indeed, throughout history, some new forms or styles of music have been criticized as ` ` not being music'', including Beethoven's Grosse Fuge string quartet in 1825, early jazz in the beginning of the 1900s and hardcore punk in the 1980s. There are many types of music, including popular music, traditional music, art music, music written for religious ceremonies and work songs such as chanteys. Music ranges from strictly organized compositions - - such as Classical music symphonies from the 1700s and 1800s, through to spontaneously played improvisational music such as jazz [SEP]\n",
      "362.9201: [CLS] The history of art is the history of any activity or product made by humans in a visual form for aesthetical or communicative purposes, expressing ideas, emotions or, in general, a worldview. Over time visual art has been classified in diverse ways, from the medieval distinction between liberal arts and mechanical arts, to the modern distinction between fine arts and applied arts, or to the many contemporary definitions, which define art as a manifestation of human creativity. The subsequent expansion of the list of principal arts in the 20th century reached to nine : architecture, dance, sculpture, music, painting, poetry - LRB - described broadly as a form of literature with aesthetic purpose or function, which also includes the distinct genres of theatre and narrative - RRB -, film, photography and graphic arts. In addition to the old forms of artistic expression such as fashion and gastronomy, new modes of expression are being considered as arts such as video, computer art, performance, advertising, animation, television and videogames. The history of art is a multidisciplinary branch of the arts and sciences, seeking an objective examination of art throughout time, classifying cultures, establishing periodizations, and observing the distinctive and influential characteristics of art. The study of the history of art was initially developed during the Renaissance, with its limited scope being the artistic production of Western civilization. However, as time has passed, it has imposed a broader view of artistic history, seeking a comprehensive overview of all the civilizations and analysis of their artistic production in terms of their own cultural values - LRB - cultural relativism - RRB -, and not just western art history. Today, art enjoys a wide network of study, dissemination and preservation of all the artistic legacy of mankind throughout history. The 20th century has seen the proliferation of institutions, foundations, art museums and galleries, in both the public and private sectors, dedicated to the analysis and cataloging of works of art as well as exhibitions aimed at a mainstream audience. The rise of media has been crucial in improving the study and dissemination of art. International events and exhibitions like the Whitney Biennial and biennales of Venice and São Paulo or the Documenta of Kassel have helped the development of new styles and trends. Prizes such as the Turner of the Tate Gallery, the Wolf Prize in Arts, the Pritzker Prize of architecture, the Pulitzer of photography and the Oscar of cinema also promote the best creative work on an international level. Institutions like UNESCO, with the establishment [SEP]\n",
      "345.6208: [CLS] Art is a diverse range of human activities in creating visual, auditory or performing artifacts - LRB - artworks - RRB -, expressing the author's imaginative or technical skill, intended to be appreciated for their beauty or emotional power. In their most general form these activities include the production of works of art, the criticism of art, the study of the history of art, and the aesthetic dissemination of art. The oldest documented forms of art are visual arts, which include creation of images or objects in fields including painting, sculpture, printmaking, photography, and other visual media. Architecture is often included as one of the visual arts ; however, like the decorative arts, or advertising, it involves the creation of objects where the practical considerations of use are essential - - in a way that they usually are not in a painting, for example. Music, theatre, film, dance, and other performing arts, as well as literature and other media such as interactive media, are included in a broader definition of art or the arts. Until the 17th century, art referred to any skill or mastery and was not differentiated from crafts or sciences. In modern usage after the 17th century, where aesthetic considerations are paramount, the fine arts are separated and distinguished from acquired skills in general, such as the decorative or applied arts. Art may be characterized in terms of mimesis - LRB - its representation of reality - RRB -, expression, communication of emotion, or other qualities. During the Romantic period, art came to be seen as ` ` a special faculty of the human mind to be classified with religion and science''. Though the definition of what constitutes art is disputed and has changed over time, general descriptions mention an idea of imaginative or technical skill stemming from human agency and creation. The nature of art and related concepts, such as creativity and interpretation, are explored in a branch of philosophy known as aesthetics. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "333.3919: [CLS] The history of science is the study of the development of science and scientific knowledge, including both the natural sciences and social sciences. - LRB - The history of the arts and humanities is termed as the history of scholarship. - RRB - Science is a body of empirical, theoretical, and practical knowledge about the natural world, produced by scientists who emphasize the observation, explanation, and prediction of real world phenomena. Historiography of science, in contrast, studies the methods by which historians study the history of science. The English word scientist is relatively recent - - first coined by William Whewell in the 19th century. Previously, people investigating nature called themselves ` ` natural philosophers''. While empirical investigations of the natural world have been described since classical antiquity - LRB - for example, by Thales and Aristotle - RRB -, and scientific method has been employed since the Middle Ages - LRB - for example, by Ibn al - Haytham and Roger Bacon - RRB -, modern science began to develop in the early modern period, and in particular in the scientific revolution of 16th - and 17th - century Europe. Traditionally, historians of science have defined science sufficiently broadly to include those earlier inquiries. From the 18th century through late 20th century, the history of science, especially of the physical and biological sciences, was often presented in a progressive narrative in which true theories replaced false beliefs. Some more recent historical interpretations, such as those of Thomas Kuhn, tend to portray the history of science in different terms, such as that of competing paradigms or conceptual systems in a wider matrix that includes intellectual, cultural, economic and political themes outside of science. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "330.5602: [CLS] The history of Science and Technology - LRB - HST - RRB - is a field of history which examines how humanity's understanding of the natural world - LRB - science - RRB - and ability to manipulate it - LRB - technology - RRB - have changed over the centuries. This academic discipline also studies the cultural, economic, and political impacts of scientific innovation. Histories of science were originally written by practicing and retired scientists, starting primarily with William Whewell, as a way to communicate the virtues of science to the public. In the early 1930s, after a famous paper given by the Soviet historian Boris Hessen, was focused into looking at the ways in which scientific practices were allied with the needs and motivations of their context. After World War II, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world. In the 1960s, especially in the wake of the work done by Thomas Kuhn, the discipline began to serve a very different function, and began to be used as a way to critically examine the scientific enterprise. At the present time it is often closely aligned with the field of science studies. Modern engineering as it is understood today took form during the scientific revolution, though much of the mathematics and science was built on the work of the Greeks, Egyptians, Mesopotamians, Chinese, Indians. See the main articles History of science and History of technology for these respective topics. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "315.1610: [CLS] History - LRB - from Greek [UNK], historia, meaning ` ` inquiry, knowledge acquired by investigation'' - RRB - is the study of the past as it is described in written documents. Events occurring before written record are considered prehistory. It is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians. History can also refer to the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing ` ` perspective'' on the problems of the present. Stories common to a particular culture, but not supported by external sources - LRB - such as the tales surrounding King Arthur - RRB -, are usually classified as cultural heritage or legends, because they do not show the ` ` disinterested investigation'' required of the discipline of history. Herodotus, a 5th - century BC Greek historian is considered within the Western tradition to be the ` ` father of history'', and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture - focused Herodotus and the military - focused Thucydides remains a point of contention or approach in modern historical writing. In Asia, a state chronicle, the Spring and Autumn Annals was known to be compiled from as early as 722 BC although only 2nd - century BC texts survived. Ancient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide - ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "299.9923: [CLS] The history of painting reaches back in time to artifacts from pre - historic humans, and spans all cultures. It represents a continuous, though periodically disrupted, tradition from Antiquity. Across cultures, and spanning continents and millennia, the history of painting is an ongoing river of creativity, that continues into the 21st century. Until the early 20th century it relied primarily on representational, religious and classical motifs, after which time more purely abstract and conceptual approaches gained favor. Developments in Eastern painting historically parallel those in Western painting, in general, a few centuries earlier. African art, Jewish art, Islamic art, Indian art, Chinese art, and Japanese art each had significant influence on Western art, and vice versa. Initially serving utilitarian purpose, followed by imperial, private, civic, and religious patronage, Eastern and Western painting later found audiences in the aristocracy and the middle class. From the Modern era, the Middle Ages through the Renaissance painters worked for the church and a wealthy aristocracy. Beginning with the Baroque era artists received private commissions from a more educated and prosperous middle class. Finally in the West the idea of ` ` art for art's sake'' began to find expression in the work of the Romantic painters like Francisco de Goya, John Constable, and J. M. W. Turner. The 19thcentury saw the rise of the commercial art gallery, which provided patronage in the 20th century. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "288.5217: [CLS] Music is found in every known culture, past and present, varying widely between times and places. Since all people of the world, including the most isolated tribal groups, have a form of music, it may be concluded that music is likely to have been present in the ancestral population prior to the dispersal of humans around the world. Consequently, music may have been in existence for at least 55, 000 years and the first music may have been invented in Africa and then evolved to become a fundamental constituent of human life. A culture's music is influenced by all other aspects of that culture, including social and economic organization and experience, climate, and access to technology. The emotions and ideas that music expresses, the situations in which music is played and listened to, and the attitudes toward music players and composers all vary between regions and periods. ` ` Music history'' is the distinct subfield of musicology and history which studies music - LRB - particularly Western art music - RRB - from a chronological perspective. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "161.2537: [CLS] The systematic provision of learning techniques to most children, such as literacy, has been a development of the last 150 or 200 years, or even last 50 years in some countries. Schools for the young have historically been supplemented with advanced training for priests, bureaucrats and specialists. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "151.2566: [CLS] The history of architecture traces the changes in architecture through various traditions, regions, overarching stylistic trends, and dates. The branches of architecture are civil, sacred, naval, military, and landscape architecture. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Query: Adrienne Bailon is an accountant.\n",
      "Top 10 passages:\n",
      "409.6913: [CLS] Adrienne Eliza Houghton - LRB - née Bailon ; born October 24, 1983 - RRB - is an American singer - songwriter, recording artist, actress, dancer and television personality. Adrienne Bailon first became publicly known when she rose to fame as one of the founding members and singer of both 3LW - LRB - 1999 - - 2007 - RRB - and Disney's The Cheetah Girls - LRB - 2003 - - 2008 - RRB -. Bailon recorded a debut solo album which was later shelved. Since then, Bailon has stated that she plans to record a Spanish - language solo album. Aside from her work in 3LW and The Cheetah Girls, Bailon went on to develop her own acting career with roles in The Cheetah Girls films, Coach Carter and the MTV film All You've Got. Aside from her work in film, she has also guest starred in numerous television series including the Disney Channel series That's So Raven and The Suite Life of Zack & Cody. Bailon has established a solo music career, contributing two solo tracks, including ` ` What If'', to The Cheetah Girls : One World soundtrack as well as the Confessions of a Shopaholic soundtrack, and numerous other guest musical ventures. Bailon co - stars alongside television personality, model and actress Julissa Bermudez in their own reality series, Empire Girls : Julissa and Adrienne which airs on the Style Network. Adrienne met Julissa on the set of the 2006 film All You've Got. Currently, Bailon is one of the hosts of syndicated talk show The Real along with former fellow Disney Channel star Tamera Mowry, Jeannie Mai, and Loni Love, which premiered on July 15, 2013. Bailon's career, spanning over 16 years with both 3LW and The Cheetah Girls, has helped her sell a combined 5. 6 million recording albums worldwide. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "332.9109: [CLS] Adrienne Lecouvreur - LRB - 5 April 1692 - - 20 March 1730 - RRB - was a French actress, considered by many as the greatest of her time. Born in Damery, she first appeared professionally on the stage in Lille. After her Paris debut at the Comédie - Française in 1717, she was immensely popular with the public. Together with Michel Baron, she was credited for having developed a more natural, less stylized, type of acting. Despite the fame she gained as an actress and her innovations in her acting style, she was widely remembered for her romance with Maurice de Saxe and for her mysterious death. Although there are different theories that suggest she was poisoned by her rival, Maria Karolina Sobieska, Duchess of Bouillon, scholars have not been able to confirm it. Her story was used as an inspiration for playwrights, composers and poets. The refusal of the Catholic Church to give her a Christian burial moved her friend Voltaire to write a poem on the subject. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "294.9057: [CLS] An accountant is a practitioner of accounting or accountancy, which is the measurement, disclosure or provision of assurance about financial information that helps managers, investors, tax authorities and others make decisions about allocating resource - LRB - s - RRB -. In many jurisdictions, professional accounting bodies maintain standards of practice and evaluations for professionals. Accountants who have demonstrated competency through their professional associations'certification exams are certified to use titles such as Chartered Accountant, Chartered Certified Accountant or Certified Public Accountant. Such professionals are granted certain responsibilities by statute, such as the ability to certify an organization's financial statements, and may be held liable for professional misconduct. Non - qualified accountants may be employed by a qualified accountant, or may work independently without statutory privileges and obligations. The Big Four auditors are the largest employers of accountants worldwide. However, most accountants are employed in commerce, industry and the public sector. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "252.2133: [CLS] Adrienne Arieff is an entrepreneur and author of several books, including the controversial book The Sacred Thread. She wrote The Sacred Thread after traveling to India and hiring a woman there to serve as a surrogate mother for her twin daughters that were conceived via in vitro fertilisation. Arieff's positive treatment of surrogacy prompted debate over the ethical and legal status of paying poor women to serve as surrogate mothers. Arieff also co - wrote with Beverly West Fairy - Tale Success : A Guide to Entrepreneurial Magic which was released in October 2014. The book is written as a manual for young women who want to run their own business. Arieff founded her business, Arieff Communications, a San Francisco - based public relations and marketing firm, in 2002. She is the sister of design writer Allison Arieff. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "155.8700: [CLS] A clerk - LRB - - LSB - klɑrk - RSB - or - LSB - [UNK] - RSB - - RRB - is a white - collar worker who conducts general office tasks, or a worker who performs similar sales - related tasks in a retail environment - LRB - a retail clerk - RRB -. The responsibilities of clerical workers commonly include record keeping, filing, staffing service counters, screening solicitors, and other administrative tasks. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "145.2906: [CLS] Adrienne Gessner - LRB - July 23, 1896 - - June 23, 1987 - RRB - was an Austrian actress. Gessner appeared in over fifty film and television shows during her career, including the 1955 costume film Royal Hunt in Ischl. Gessner appeared in a mixture of German and Austrian films during her career. Following the Anchluss of 1938 she fled with her Jewish husband Ernst Lothar to the United States, returning after the Second World War. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "125.4794: [CLS] Adrienne A. Jones - LRB - born November 20, 1954 - RRB - is the current Speaker Pro Tem of the Maryland House of Delegates, the first African - American female to serve in that position in Maryland. She was appointed by Governor Parris Glendening to fill the vacancy created by the death of Delegate Joan Neverdonn Parker in 1997. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "122.1903: [CLS] Adrienne C. Moore - LRB - born August 14, 1980 - RRB - is an American actress, known for her role as Cindy ` ` Black Cindy'' Hayes in the Netflix comedy - drama series Orange Is the New Black. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "112.7224: [CLS] Adrienne is the French feminine form of the male name Adrien. Its meaning is literally ` ` from the city Hadria.'' It also means ` ` the dark one''. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "107.8892: [CLS] Adieu - LRB - French for ` ` farewell'' - RRB - may refer to : [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Query: Homeland is an American television spy thriller based on the Israeli television series Prisoners of War.\n",
      "Top 10 passages:\n",
      "401.5815: [CLS] Homeland is an American spy thriller television series developed by Howard Gordon and Alex Gansa based on the Israeli series Prisoners of War - LRB - Original title חטופים Hatufim, literally ` ` Abductees'' - RRB -, which was created by Gideon Raff. The series stars Claire Danes as Carrie Mathison, a Central Intelligence Agency officer with bipolar disorder, and Damian Lewis as Nicholas Brody, a U. S. Marine Corps Scout Sniper. Mathison had come to believe that Brody, who was held captive by al - Qaeda as a prisoner of war, was ` ` turned'' by the enemy and poses a threat to the United States. The series is broadcast in the U. S. on the cable channel Showtime, and is produced by Fox 21 Television Studios - LRB - formerly Fox 21 - RRB -. It premiered on October 2, 2011. The first episode was made available online, more than two weeks before the television broadcast, with viewers having to complete game tasks to gain access. On October 22, 2013, Showtime renewed Homeland for a fourth season, which premiered on October 5, 2014. On November 10, 2014, Showtime renewed the series for a 12 - episode fifth season that premiered on October 4, 2015. On December 9, 2015, the series was renewed for a sixth season. The sixth season debuted on January 15, 2017. The series has also been renewed for a seventh and eighth season ; the eighth is planned to be the series'final season by the creators. The series has received generally positive reviews, and has won several awards, including the 2012 Primetime Emmy Award for Outstanding Drama Series, and the 2011 and 2012 Golden Globe Award for Best Television Series - - Drama, as well as the Primetime Emmy Award for Outstanding Lead Actor in a Drama Series and Lead Actress in a Drama Series for Damian Lewis and Claire Danes, respectively. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "387.1860: [CLS] House - LRB - also called House, M. D. - RRB - is an American television medical drama that originally ran on the Fox network for eight seasons, from November 16, 2004 to May 21, 2012. The series'main character is Dr. Gregory House - LRB - Hugh Laurie - RRB -, an unconventional, misanthropic medical genius who, despite his dependence on pain medication, leads a team of diagnosticians at the fictional Princeton - - Plainsboro Teaching Hospital - LRB - PPTH - RRB - in New Jersey. The series'premise originated with Paul Attanasio, while David Shore, who is credited as creator, was primarily responsible for the conception of the title character. The series'executive producers included Shore, Attanasio, Attanasio's business partner Katie Jacobs, and film director Bryan Singer. It was filmed largely in Century City. House often clashes with his fellow physicians, including his own diagnostic team, because many of his hypotheses about patients'illnesses are based on subtle or controversial insights. His flouting of hospital rules and procedures frequently leads him into conflict with his boss, hospital administrator and Dean of Medicine Dr. Lisa Cuddy - LRB - Lisa Edelstein - RRB -. House's only true friend is Dr. James Wilson - LRB - Robert Sean Leonard - RRB -, head of the Department of Oncology. During the first three seasons, House's diagnostic team consists of Dr. Robert Chase - LRB - Jesse Spencer - RRB -, Dr. Allison Cameron - LRB - Jennifer Morrison - RRB -, and Dr. Eric Foreman - LRB - Omar Epps - RRB -. At the end of the third season, this team disbands. Rejoined by Foreman, House gradually selects three new team members : Dr. Remy ` ` Thirteen'' Hadley - LRB - Olivia Wilde - RRB -, Dr. Chris Taub - LRB - Peter Jacobson - RRB -, and Dr. Lawrence Kutner - LRB - Kal Penn - RRB -. Kutner makes an appearance late in season five and then reappears in season 8 episode 22. Chase and Cameron continue to appear in different roles at the hospital until early in season six. Cameron then departs the hospital, and Chase returns to the diagnostic team. Thirteen takes a leave of absence for most of season seven, and her position is [SEP]\n",
      "358.4518: [CLS] The first season of the American television drama series Homeland premiered on October 2, 2011 on Showtime and concluded on December 18, 2011, consisting of 12 episodes. The series is loosely based on the Israeli television series Hatufim - LRB - English : Prisoners of War - RRB - created by Gideon Raff and is developed for American television by Howard Gordon and Alex Gansa. The first season follows Carrie Mathison, a CIA operations officer who has come to believe that Nicholas Brody, a U. S. Marine Sergeant, who was held captive by al - Qaeda as a prisoner of war, was turned by the enemy and now poses a significant risk to national security. The season received universal acclaim, scoring a Metacritic rating of 91 out of 100 from 28 critics. TV Guide named it the best TV show of 2011 and highly applauded the performances by Damian Lewis and Claire Danes. Metacritic determined Homeland to be the second - best TV show of 2011 according to major TV critics, by aggregating the critics'year - end top ten lists. The series won both the Golden Globe Award for Best Television Series - - Drama and the Primetime Emmy Award for Outstanding Drama Series for this season. The original broadcast of the pilot episode on October 2, 2011 received 1. 08 million viewers, becoming Showtime's highest - rated drama premiere in eight years. The episode received a total of 2. 78 million viewers with additional broadcasts and on demand views. The finale episode of season one received 1. 7 million viewers, making it the most - watched season finale of any first - year Showtime series. The series also performed well in the UK, where it aired on Channel 4, with the pilot episode drawing 3. 10 million viewers, and the finale drawing 4. 01 million viewers. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "215.6153: [CLS] The fifth season of the American television drama series Homeland, premiered on October 4, 2015, and concluded on December 20, 2015, on Showtime, consisting of 12 episodes. The series is loosely based on the Israeli television series Hatufim - LRB - English : Prisoners of War - RRB - created by Gideon Raff and is developed for American television by Howard Gordon and Alex Gansa. The fifth season was released on Blu - ray and DVD on January 10, 2017. Set two years after the previous season, Carrie is no longer working for the CIA, and is working for a philanthropic foundation in Berlin, the Düring Foundation. The season includes several real world subjects in its storylines, including ISIS, Vladimir Putin, Bashar al - Assad, the Charlie Hebdo shooting, Edward Snowden, and the European migrant crisis. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "194.0026: [CLS] Prisoners of War - LRB - original title in - RRB - is an Israeli television drama series made by Keshet and originally aired on Israel's Channel 2 from March to May 2010. A second season aired in Israel from October to December 2012, and a third season is planned. The series was created by Israeli director, screenwriter and producer Gideon Raff. In 2010 it won the Israeli Academy Award for Television for Best Drama Series. The programme was acquired by 20th Century Fox Television before it aired in Israel, and was adapted into the acclaimed series Homeland for Showtime in the United States. In India it was officially adapted as finite 126 episode television series P. O. W. - Bandi Yuddh Ke by Nikhil Advani in 2016 with Hatufim creator Gideon Raff consulting for the series. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "191.9635: [CLS] A homeland - LRB - country of origin and native land - RRB - is the concept of the place - LRB - cultural geography - RRB - with which an ethnic group holds a long history and a deep cultural association - - the country in which a particular national identity began. As a common noun, homeland, it simply connotes the country of one's origin. When used as a proper noun, the Homeland, as well as its equivalents in other languages, often have ethnic nationalist connotations. A homeland may also be referred to as a fatherland, a motherland, or a mother country, depending on the culture and language of the nationality in question. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "143.6770: [CLS] Homeland - LRB - [UNK] [UNK], translit. Ieji - RRB - is a 2014 Japanese drama film directed by Nao Kubota. The film had its premiere in the Panorama section of the 64th Berlin International Film Festival. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "123.8435: [CLS] The fourth season of the American television drama series Homeland premiered on October 5, 2014, and concluded on December 21, 2014, on Showtime, consisting of 12 episodes. The series is loosely based on the Israeli television series Hatufim - LRB - English : Prisoners of War - RRB - created by Gideon Raff and is developed for American television by Howard Gordon and Alex Gansa. The fourth season was released on Blu - ray and DVD on September 8, 2015, and became available for streaming on Hulu on August 1, 2016. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "92.9942: [CLS] The third season of the American television drama series Homeland premiered on September 29, 2013 on Showtime, and concluded on December 15, 2013, consisting of 12 episodes. The series is loosely based on the Israeli television series Hatufim - LRB - English : Prisoners of War - RRB - created by Gideon Raff and is developed for American television by Howard Gordon and Alex Gansa. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "77.8632: [CLS] Prisoners of War is an album by rapper Sun Rise Above. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_testing.eval()\n",
    "with open(\"./fever_data/fever_retrieval_train.jsonl\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 4:\n",
    "            break\n",
    "\n",
    "        inst = json.loads(line)\n",
    "        query = inst[\"query\"]\n",
    "        passages = [list(d.values())[0] for d in inst[\"positive_passages\"][0]] + [\n",
    "            list(d.values())[0] for d in inst[\"negative_passages\"][0]\n",
    "        ]\n",
    "\n",
    "        query_encoding = tokenizer.encode(\n",
    "            query,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        passages_encoding = [\n",
    "            tokenizer.encode(\n",
    "                passage,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "            for passage in passages\n",
    "        ]\n",
    "\n",
    "        query_output = model_testing(\n",
    "            torch.tensor(query_encoding).unsqueeze(0).to(device)\n",
    "        )\n",
    "        passages_output = model_testing(torch.tensor(passages_encoding).to(device))\n",
    "\n",
    "        inner_product = torch.matmul(query_output[1], passages_output[1].t()).squeeze()\n",
    "        ranked_passages = torch.argsort(inner_product, descending=True).tolist()\n",
    "\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"Top 10 passages:\")\n",
    "        for passage_idx in ranked_passages[:10]:\n",
    "            print(\n",
    "                f\"{inner_product[passage_idx].item():.4f}: {tokenizer.decode(passages_encoding[passage_idx])}\"\n",
    "            )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9240b7",
   "metadata": {},
   "source": [
    "**Problem 2.5** (8 points) Using the provided FEVER dataset, and extending your answer from 2.4, implement the training loop for DPR by fine-tuning the HuggingFace `bert-base-cased` model. Implement the G+BM25 31+32 setting (see table 3).\n",
    "\n",
    "- Plot the training and validation loss curves.\n",
    "- Plot the recall@k curve on the final checkpoint\n",
    "- To simulate the BM25 negatives, sample a passage from the list of provided negatives from the FEVER file.\n",
    "- You may have slightly different results as the provided negative passages in the FEVER file are more challenging than BM25.\n",
    "- There is a bug in reproducing the DPR paper, it may be necessary to perform unit normalization of the BERT embeddings if the exponent of the inner product gets too high: https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html\n",
    "\n",
    "Rubric:\n",
    "\n",
    "- Up to 4 points depending on implementation complexity. Computing the loss of a minibatch of strings. If you compute this 1-by-1, only partial credit is possible.\n",
    "- 2 points for code for loading the data and training the model\n",
    "- 1 point for convergence of the model and plotting training/validation loss\n",
    "- 1 point for plotting recall@k on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06c7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10969107",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11d2c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71685d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save(file, tokenizer, save_file_name, total_max_len=512):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    num_queries = len(lines)\n",
    "    nonempty_passage_count = 0\n",
    "\n",
    "    all_query_tokens = torch.zeros((num_queries, total_max_len), dtype=torch.int32)\n",
    "    all_positive_tokens = torch.zeros((num_queries, total_max_len), dtype=torch.int32)\n",
    "    all_negative_tokens = torch.zeros((num_queries, total_max_len), dtype=torch.int32)\n",
    "\n",
    "    for line in tqdm(lines):\n",
    "        inst = json.loads(line)\n",
    "\n",
    "        try:\n",
    "            query = inst[\"query\"]\n",
    "            positive_passage = list(inst[\"positive_passages\"][0][0].values())[0]\n",
    "            negative_passage = list(inst[\"negative_passages\"][0][0].values())[0]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        query_tokens = tokenizer.encode(\n",
    "            query,\n",
    "            add_special_tokens=True,\n",
    "            max_length=total_max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        positive_tokens = tokenizer.encode(\n",
    "            positive_passage,\n",
    "            add_special_tokens=True,\n",
    "            max_length=total_max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        negative_tokens = tokenizer.encode(\n",
    "            negative_passage,\n",
    "            add_special_tokens=True,\n",
    "            max_length=total_max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        all_query_tokens[nonempty_passage_count] = torch.tensor(query_tokens)\n",
    "        all_positive_tokens[nonempty_passage_count] = torch.tensor(positive_tokens)\n",
    "        all_negative_tokens[nonempty_passage_count] = torch.tensor(negative_tokens)\n",
    "\n",
    "        nonempty_passage_count += 1\n",
    "\n",
    "    del lines\n",
    "\n",
    "    all_query_tokens = all_query_tokens[:nonempty_passage_count]\n",
    "    all_positive_tokens = all_positive_tokens[:nonempty_passage_count]\n",
    "    all_negative_tokens = all_negative_tokens[:nonempty_passage_count]\n",
    "\n",
    "    assert all_positive_tokens.shape == all_negative_tokens.shape\n",
    "\n",
    "    torch.save(\n",
    "        (all_query_tokens, all_positive_tokens, all_negative_tokens),\n",
    "        save_file_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f301bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109810/109810 [04:35<00:00, 399.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [00:13<00:00, 503.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [00:14<00:00, 467.12it/s]\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\n",
    "    \"./fever_data/fever_retrieval_train.jsonl\",\n",
    "    \"./fever_data/fever_retrieval_dev.jsonl\",\n",
    "    \"./fever_data/fever_retrieval_test.jsonl\",\n",
    "]\n",
    "names = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "for name, path in zip(names, filepaths):\n",
    "    save_file_name = f\"./fever_data/fever_dpr_{name}.pt\"\n",
    "    print(f\"Preprocessing {name} data...\")\n",
    "    preprocess_and_save(path, tokenizer, save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be71471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        (\n",
    "            self.query_tokens,\n",
    "            self.positive_tokens,\n",
    "            self.negative_tokens,\n",
    "        ) = torch.load(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.query_tokens.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query_token = self.query_tokens[idx]\n",
    "        positive_token = self.positive_tokens[idx]\n",
    "        negative_token = self.negative_tokens[idx]\n",
    "\n",
    "        return query_token, positive_token, negative_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d894f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DPRDataset(\"./fever_data/fever_dpr_train.pt\")\n",
    "test_dataset = DPRDataset(\"./fever_data/fever_dpr_test.pt\")\n",
    "val_dataset = DPRDataset(\"./fever_data/fever_dpr_dev.pt\")\n",
    "\n",
    "# I tried training with batch size 32, but this is the maximum that fits in my GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5464c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_q = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.model_p = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "        self.model_q.to(device)\n",
    "        self.model_p.to(device)\n",
    "\n",
    "    def forward(self, query_tokens, passage_tokens):\n",
    "        \"\"\"\n",
    "        query_tokens: [B, 512]\n",
    "        passage_tokens: [1 + (B-1) + B, 512]\n",
    "        \"\"\"\n",
    "        query_attention_mask = (query_tokens != 0).int()\n",
    "        passage_attention_mask = (passage_tokens != 0).int()\n",
    "\n",
    "        query_outputs = self.model_q(query_tokens, attention_mask=query_attention_mask)[\n",
    "            1\n",
    "        ]\n",
    "        passage_outputs = self.model_p(\n",
    "            passage_tokens, attention_mask=passage_attention_mask\n",
    "        )[1]\n",
    "\n",
    "        scores = torch.matmul(query_outputs, passage_outputs.t())  # [32, 64]\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f396a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPRModel()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "def criterion(scores, targets):\n",
    "    # I did not use normalisation because the operations used are numerically stable\n",
    "    softmax_scores = F.log_softmax(scores, dim=-1)\n",
    "\n",
    "    loss = F.nll_loss(\n",
    "        softmax_scores,\n",
    "        targets,\n",
    "        reduction=\"mean\",\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf0771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "max_training_time = 2 * 60 * 60  # 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a897fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(tqdm(val_loader)):\n",
    "            query_tokens, positive_tokens, negative_tokens = batch\n",
    "            batch_size = query_tokens.shape[0]\n",
    "\n",
    "            query_tokens = query_tokens.to(device)\n",
    "            passage_tokens = torch.cat((positive_tokens, negative_tokens), dim=0).to(\n",
    "                device\n",
    "            )\n",
    "            target_tensor = torch.arange(batch_size).to(device)\n",
    "\n",
    "            scores = model(query_tokens, passage_tokens)\n",
    "            total_val_loss += criterion(scores, target_tensor)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"ai605-assignment3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2601b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 0, Training Loss: 48.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:34<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 0, Validation Loss: 34.1752\n",
      "Epoch 1/5, Iteration 100, Training Loss: 2.1952\n",
      "Epoch 1/5, Iteration 200, Training Loss: 1.5538\n",
      "Epoch 1/5, Iteration 300, Training Loss: 0.3324\n",
      "Epoch 1/5, Iteration 400, Training Loss: 0.4793\n",
      "Epoch 1/5, Iteration 500, Training Loss: 0.5454\n",
      "Epoch 1/5, Iteration 600, Training Loss: 0.4194\n",
      "Epoch 1/5, Iteration 700, Training Loss: 0.2053\n",
      "Epoch 1/5, Iteration 800, Training Loss: 0.2283\n",
      "Epoch 1/5, Iteration 900, Training Loss: 0.1021\n",
      "Epoch 1/5, Iteration 1000, Training Loss: 0.2228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:33<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 1000, Validation Loss: 0.4663\n",
      "Epoch 1/5, Iteration 1100, Training Loss: 0.0427\n",
      "Epoch 1/5, Iteration 1200, Training Loss: 0.0728\n",
      "Epoch 1/5, Iteration 1300, Training Loss: 0.2026\n",
      "Epoch 1/5, Iteration 1400, Training Loss: 0.2919\n",
      "Epoch 1/5, Iteration 1500, Training Loss: 0.0184\n",
      "Epoch 1/5, Iteration 1600, Training Loss: 0.0274\n",
      "Epoch 1/5, Iteration 1700, Training Loss: 0.1638\n",
      "Epoch 1/5, Iteration 1800, Training Loss: 0.9190\n",
      "Epoch 1/5, Iteration 1900, Training Loss: 0.0315\n",
      "Epoch 1/5, Iteration 2000, Training Loss: 0.0239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:33<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 2000, Validation Loss: 0.3003\n",
      "Epoch 1/5, Iteration 2100, Training Loss: 0.0359\n",
      "Epoch 1/5, Iteration 2200, Training Loss: 0.2498\n",
      "Epoch 1/5, Iteration 2300, Training Loss: 0.3500\n",
      "Epoch 1/5, Iteration 2400, Training Loss: 0.0281\n",
      "Epoch 1/5, Iteration 2500, Training Loss: 0.0205\n",
      "Epoch 1/5, Iteration 2600, Training Loss: 0.0332\n",
      "Epoch 1/5, Iteration 2700, Training Loss: 0.6684\n",
      "Epoch 1/5, Iteration 2800, Training Loss: 0.0181\n",
      "Epoch 1/5, Iteration 2900, Training Loss: 0.0752\n",
      "Epoch 1/5, Iteration 3000, Training Loss: 0.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:33<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 3000, Validation Loss: 0.3091\n",
      "Epoch 1/5, Iteration 3100, Training Loss: 0.4103\n",
      "Epoch 1/5, Iteration 3200, Training Loss: 0.0143\n",
      "Epoch 1/5, Iteration 3300, Training Loss: 0.0201\n",
      "Epoch 1/5, Iteration 3400, Training Loss: 0.4588\n",
      "Epoch 1/5, Iteration 3500, Training Loss: 0.0322\n",
      "Epoch 1/5, Iteration 3600, Training Loss: 0.0189\n",
      "Epoch 1/5, Iteration 3700, Training Loss: 0.2488\n",
      "Epoch 1/5, Iteration 3800, Training Loss: 0.0128\n",
      "Epoch 1/5, Iteration 3900, Training Loss: 0.0185\n",
      "Epoch 1/5, Iteration 4000, Training Loss: 0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:34<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 4000, Validation Loss: 0.3994\n",
      "Epoch 1/5, Iteration 4100, Training Loss: 0.0091\n",
      "Epoch 1/5, Iteration 4200, Training Loss: 0.0099\n",
      "Epoch 1/5, Iteration 4300, Training Loss: 0.0659\n",
      "Epoch 1/5, Iteration 4400, Training Loss: 1.5395\n",
      "Epoch 1/5, Iteration 4500, Training Loss: 0.0075\n",
      "Epoch 1/5, Iteration 4600, Training Loss: 0.0832\n",
      "Epoch 1/5, Iteration 4700, Training Loss: 0.0333\n",
      "Epoch 1/5, Iteration 4800, Training Loss: 0.0108\n",
      "Epoch 1/5, Iteration 4900, Training Loss: 0.0576\n",
      "Epoch 1/5, Iteration 5000, Training Loss: 0.0574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:34<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 5000, Validation Loss: 0.3531\n",
      "Epoch 1/5, Iteration 5100, Training Loss: 0.0505\n",
      "Epoch 1/5, Iteration 5200, Training Loss: 0.0062\n",
      "Epoch 1/5, Iteration 5300, Training Loss: 0.0168\n",
      "Epoch 1/5, Iteration 5400, Training Loss: 0.0056\n",
      "Epoch 1/5, Iteration 5500, Training Loss: 0.0230\n",
      "Epoch 1/5, Iteration 5600, Training Loss: 0.2233\n",
      "Epoch 1/5, Iteration 5700, Training Loss: 0.2613\n",
      "Epoch 1/5, Iteration 5800, Training Loss: 0.0129\n",
      "Epoch 1/5, Iteration 5900, Training Loss: 0.0840\n",
      "Epoch 1/5, Iteration 6000, Training Loss: 0.0458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:34<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 6000, Validation Loss: 0.3522\n",
      "Epoch 1/5, Iteration 6100, Training Loss: 0.1734\n",
      "Epoch 1/5, Iteration 6200, Training Loss: 0.4629\n",
      "Epoch 1/5, Iteration 6300, Training Loss: 0.0658\n",
      "Epoch 1/5, Iteration 6400, Training Loss: 0.0510\n",
      "Epoch 1/5, Iteration 6500, Training Loss: 0.0117\n",
      "Epoch 1/5, Iteration 6600, Training Loss: 0.9099\n",
      "Epoch 1/5, Iteration 6700, Training Loss: 0.0323\n",
      "Epoch 1/5, Iteration 6800, Training Loss: 0.1013\n",
      "Epoch 1/5, Iteration 6900, Training Loss: 0.0345\n",
      "Epoch 1/5, Iteration 7000, Training Loss: 0.3895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:34<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 7000, Validation Loss: 0.4599\n",
      "Epoch 1/5, Iteration 7100, Training Loss: 0.0296\n",
      "Epoch 1/5, Iteration 7200, Training Loss: 0.0127\n",
      "Epoch 1/5, Iteration 7300, Training Loss: 0.0025\n",
      "Epoch 1/5, Iteration 7400, Training Loss: 0.0732\n",
      "Epoch 1/5, Iteration 7500, Training Loss: 0.0038\n",
      "Epoch 1/5, Iteration 7600, Training Loss: 0.0515\n",
      "Epoch 1/5, Iteration 7700, Training Loss: 0.0187\n",
      "Epoch 1/5, Iteration 7800, Training Loss: 0.0684\n",
      "Epoch 1/5, Iteration 7900, Training Loss: 0.0343\n",
      "Epoch 1/5, Iteration 8000, Training Loss: 0.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:33<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 8000, Validation Loss: 0.5250\n",
      "Epoch 1/5, Iteration 8100, Training Loss: 0.2242\n",
      "Epoch 1/5, Iteration 8200, Training Loss: 0.8390\n",
      "Epoch 1/5, Iteration 8300, Training Loss: 0.1620\n",
      "Epoch 1/5, Iteration 8400, Training Loss: 0.2210\n",
      "Epoch 1/5, Iteration 8500, Training Loss: 0.0258\n",
      "Epoch 1/5, Iteration 8600, Training Loss: 0.0151\n",
      "Epoch 1/5, Iteration 8700, Training Loss: 0.0114\n",
      "Epoch 1/5, Iteration 8800, Training Loss: 0.3716\n",
      "Epoch 1/5, Iteration 8900, Training Loss: 0.1533\n",
      "Epoch 1/5, Iteration 9000, Training Loss: 0.1491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:34<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Iteration 9000, Validation Loss: 0.5173\n",
      "Epoch 1/5, Iteration 9100, Training Loss: 0.3991\n",
      "Epoch 1/5, Iteration 9200, Training Loss: 0.1563\n",
      "Epoch 1/5, Iteration 9300, Training Loss: 0.1156\n",
      "Epoch 1/5, Iteration 9400, Training Loss: 0.3608\n",
      "Epoch 1/5, Iteration 9500, Training Loss: 0.1605\n",
      "Epoch 1/5, Iteration 9600, Training Loss: 0.0083\n",
      "Epoch 2/5, Iteration 0, Training Loss: 0.5068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:33<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Iteration 0, Validation Loss: 0.7229\n",
      "Epoch 2/5, Iteration 100, Training Loss: 0.0968\n",
      "Epoch 2/5, Iteration 200, Training Loss: 0.0211\n",
      "Epoch 2/5, Iteration 300, Training Loss: 0.0581\n",
      "Epoch 2/5, Iteration 400, Training Loss: 0.0060\n",
      "Epoch 2/5, Iteration 500, Training Loss: 0.0117\n",
      "Epoch 2/5, Iteration 600, Training Loss: 0.0047\n",
      "Epoch 2/5, Iteration 700, Training Loss: 0.6724\n",
      "Epoch 2/5, Iteration 800, Training Loss: 0.0476\n",
      "Epoch 2/5, Iteration 900, Training Loss: 0.0358\n",
      "Epoch 2/5, Iteration 1000, Training Loss: 0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:33<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Iteration 1000, Validation Loss: 0.9969\n",
      "Epoch 2/5, Iteration 1100, Training Loss: 0.0479\n",
      "Epoch 2/5, Iteration 1200, Training Loss: 0.0209\n",
      "Epoch 2/5, Iteration 1300, Training Loss: 0.2255\n",
      "Epoch 2/5, Iteration 1400, Training Loss: 0.4462\n",
      "Epoch 2/5, Iteration 1500, Training Loss: 0.7432\n",
      "Epoch 2/5, Iteration 1600, Training Loss: 0.5625\n",
      "Epoch 2/5, Iteration 1700, Training Loss: 0.2761\n",
      "Epoch 2/5, Iteration 1800, Training Loss: 0.0145\n",
      "Epoch 2/5, Iteration 1900, Training Loss: 0.0112\n",
      "Epoch 2/5, Iteration 2000, Training Loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [01:34<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Iteration 2000, Validation Loss: 0.9944\n",
      "Reached maximum training time. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for train_iter, batch in enumerate(train_loader):\n",
    "        current_time = time.time()\n",
    "        if (current_time - start_time) > max_training_time:\n",
    "            break\n",
    "\n",
    "        query_tokens, positive_tokens, negative_tokens = batch\n",
    "        batch_size = query_tokens.shape[0]\n",
    "\n",
    "        query_tokens = query_tokens.to(device)\n",
    "        passage_tokens = torch.cat((positive_tokens, negative_tokens), dim=0).to(device)\n",
    "        target_tensor = torch.arange(batch_size).to(device)\n",
    "\n",
    "        scores = model(query_tokens, passage_tokens)\n",
    "        loss = criterion(scores, target_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"Training Loss\": loss})\n",
    "\n",
    "        if train_iter % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs}, Iteration {train_iter}, Training Loss: {loss:.4f}\"\n",
    "            )\n",
    "\n",
    "        if train_iter % 1000 == 0:\n",
    "            val_loss = evaluate_model(model, val_loader, device)\n",
    "            wandb.log({\"Validation Loss\": val_loss})\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs}, Iteration {train_iter}, Validation Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss,\n",
    "        },\n",
    "        f\"./dpr_epoch{epoch}.pth\",\n",
    "    )\n",
    "\n",
    "    if (time.time() - start_time) > max_training_time:\n",
    "        print(\"Reached maximum training time. Stopping training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94c1eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained models.\n"
     ]
    }
   ],
   "source": [
    "model_p_trained = model.model_p\n",
    "model_q_trained = model.model_q\n",
    "\n",
    "model_p_trained.eval()\n",
    "model_q_trained.eval()\n",
    "print(\"Loaded trained models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66d4ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_passage_encodings(model_p, passage_tokens):\n",
    "    passage_attention_mask = (passage_tokens != 0).int()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        passage_outputs = model_p(\n",
    "            passage_tokens, attention_mask=passage_attention_mask\n",
    "        )[1]\n",
    "\n",
    "    return passage_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7fdac985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/136 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [01:03<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "num_passages = len(test_dataset)\n",
    "\n",
    "all_positive_encodings = torch.zeros((num_passages, 768), dtype=torch.float32)\n",
    "all_negative_encodings = torch.zeros((num_passages, 768), dtype=torch.float32)\n",
    "\n",
    "for i, batch in enumerate(tqdm(test_loader)):\n",
    "    _, positive_tokens, negative_tokens = batch\n",
    "    curr_batch_size = positive_tokens.shape[0]\n",
    "\n",
    "    positive_tokens = positive_tokens.to(device)\n",
    "    negative_tokens = negative_tokens.to(device)\n",
    "\n",
    "    positive_encodings = precompute_passage_encodings(model_p_trained, positive_tokens)\n",
    "    negative_encodings = precompute_passage_encodings(model_p_trained, negative_tokens)\n",
    "\n",
    "    all_positive_encodings[i * 32 : i * 32 + curr_batch_size] = positive_encodings\n",
    "    all_negative_encodings[i * 32 : i * 32 + curr_batch_size] = negative_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8664, 768])\n"
     ]
    }
   ],
   "source": [
    "all_passage_encodings = torch.cat(\n",
    "    (all_positive_encodings, all_negative_encodings), dim=0\n",
    ")\n",
    "all_passage_encodings = all_passage_encodings.to(device)\n",
    "print(all_passage_encodings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "002620e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_at_k(model_q, test_loader, test_passage_encodings, k_values):\n",
    "    recall_scores = {k: 0 for k in k_values}\n",
    "\n",
    "    model_q.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_loader)):\n",
    "            query_tokens, _, _ = batch\n",
    "            curr_batch_size = query_tokens.shape[0]\n",
    "\n",
    "            query_tokens = query_tokens.to(device)\n",
    "            query_attention_mask = (query_tokens != 0).int()\n",
    "            query_encodings = model_q(\n",
    "                query_tokens, attention_mask=query_attention_mask\n",
    "            )[1]\n",
    "\n",
    "            scores = torch.matmul(\n",
    "                query_encodings, test_passage_encodings.t()\n",
    "            )  # shape: [32, M]\n",
    "            ranks = torch.argsort(scores, descending=True, dim=1) + 1\n",
    "            curr_batch_portion = ranks[:, i * 32 : i * 32 + curr_batch_size]\n",
    "            correct_ranks = torch.diagonal(curr_batch_portion)\n",
    "            assert correct_ranks.shape[0] == curr_batch_size\n",
    "\n",
    "            for k in k_values:\n",
    "                recall_scores[k] += torch.sum(correct_ranks <= k).item()\n",
    "\n",
    "    average_recall = {k: recall_scores[k] / len(test_dataset) for k in k_values}\n",
    "    return average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b21fa72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [00:31<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "k_values = [500, 1000, 2000, 4000]\n",
    "recall_at_k = calculate_recall_at_k(\n",
    "    model_q_trained, test_loader, all_passage_encodings, k_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e2e1ef3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_list = list(recall_at_k.keys())\n",
    "recall_list = list(recall_at_k.values())\n",
    "\n",
    "k_categories = [str(k) for k in k_list]\n",
    "x_positions = range(len(k_categories))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_positions, recall_list, marker=\"o\")\n",
    "plt.xticks(x_positions, k_categories)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall@k\")\n",
    "plt.title(\"Recall@k on Test Set\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9dae61",
   "metadata": {},
   "source": [
    "# Bonus Problem - \"KAIST Questions for Questions Scheme\" (5 bonus points)\n",
    "\n",
    "“Questions for Questions” has been implemented to foster talent capable of asking questions and to shape the QAIST DNA, in line with the QAIST New Culture Strategy. Instructors allow students to create their own questions for examinations. By enhancing students’ ability to independently think and question, the university seeks to develop a culture of nurturing creative talent.\n",
    "\n",
    "“문제내는 문제”는 QAIST 신문화전략 중 질문하는 인재! QAIST DNA 형성을 위한 실천 방안으로 매학기 개설되는 정규수업에서 강의교원이 답 없는 문제를 출제하거나,\n",
    "학생 스스로가 직접 문제를 출제하도록 하는 시험 방식입니다. 이를 통해 학생 스스로 사고하고 질문하는 역량을 키워줌으로써 창의 인재를 양성하는 문화로 확산해 나가고자 합니다.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Please formulate one question (and include a sample answer/rubric) that you would propose for an NLP exam. A selection of high quality questions will be featured in the 4th assignment as an additional bonus question.\n",
    "\n",
    "Your question should ideally pertain to topics discussed in the recommended readings, lecture slides, or practical exercises. It is suggested that you base your question on a topic where you previously lost marks in the first three assignments. This will not only provide an opportunity for you to deepen your understanding of the subject but also assist your peers in revisiting these essential concepts.\n",
    "\n",
    "Tips for Making your Question:\n",
    "\n",
    "- Relevance: Ensure the question is tightly aligned with course content and objectives.\n",
    "- Clarity: Formulate your question clearly and unambiguously.\n",
    "- Depth: Aim for a question that tests more than just factual recall – include application, analysis, or evaluation.\n",
    "- Innovative Thinking: Try to introduce a new perspective or a unique problem-solving approach.\n",
    "- Constructive Learning: Focus on areas that can enhance understanding of complex topics or correct common misconceptions.\n",
    "\n",
    "### Judging criteria:\n",
    "\n",
    "Please follow the following criteria\n",
    "\n",
    "- True/False, Multiple Choice, or Simple Bookwork questions will not be accepted.\n",
    "- The question must be directly related to topics covered in the readings, lectures, or exercises, or be applications of the course material.\n",
    "- The question can be complex book work, connecting difffernt topics, coding or math based\n",
    "- It should reflect important concepts or skills that are fundamental to the course.\n",
    "- The question should be clearly phrased and unambiguous.\n",
    "- The sample answer / rubric should be correct.\n",
    "- It must be structured logically, allowing students to understand what is being asked without confusion.\n",
    "\n",
    "The following grading rubric will be applied\n",
    "\n",
    "- **Depth and Complexity**\n",
    "\n",
    "  - The question should challenge students to think critically or apply concepts in a novel way.\n",
    "  - Higher points for questions that require synthesis of multiple concepts or analysis of scenarios.\n",
    "  - If this question relates to where you lost marks in the first 2 assignments, the complexity score will be easier to attain\n",
    "\n",
    "- **Originality and Creativity**\n",
    "\n",
    "  - The question should offer a fresh perspective or tackle a common topic in an innovative way.\n",
    "  - It should stand out from standard textbook questions or those commonly found in exams.\n",
    "\n",
    "- **Practical Application or Implication**\n",
    "\n",
    "  - Questions that connect theory to real-world applications or current research trends score higher.\n",
    "  - The question should prompt students to think about the practical implications or future directions of the topic.\n",
    "\n",
    "- **Correctness**\n",
    "  - The sample answer should be correct to attain the grade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb685bc",
   "metadata": {},
   "source": [
    "**Question 1**: When implementing the softmax function in machine learning models, one can encounter specific problematic scenarios. What are the problems that arises with softmax, particularly related to numerical stability? Additionally, how would you address this issue to ensure the stable operation of the softmax function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c09f4f",
   "metadata": {},
   "source": [
    "One of the main issues with the softmax function is numerical instability, especially when dealing with very large or very small numbers. This is because softmax involves exponentiation, which can lead to extremely large values for big input numbers, causing an overflow (or very small values for small input numbers, leading to underflow). Both situations can result in numerical errors or imprecise calculations.\n",
    "\n",
    "To address this, a common technique is to use the \"softmax stabilisation\" method. This involves subtracting the maximum value in the input vector from each element of the vector before applying the exponentiation:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i - M}}{\\sum_j e^{x_j - M}}, \\text{ where } M = \\max_i{x_i}\n",
    "$$\n",
    "\n",
    "This does not affect the output of the softmax function, as it gets canceled out in the normalization step. It also significantly reduces the chances of encountering overflow or underflow problems. This technique ensures more numerically stable computations without altering the softmax function's output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e2f37",
   "metadata": {},
   "source": [
    "**Question 2**: Recall the objective function used in DPR. How could you modify this objective function to introduce a control mechanism for the penalty applied to hard negatives? (A hard negative is defined as a non-relevant passage that is similar enough to a query to be mistaken as relevant.) How would this modification impact the training and performance of the DPR model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22bf81",
   "metadata": {},
   "source": [
    "To introduce a control mechanism for the penalty applied to hard negatives, we can implement a temperature hyperparameter in the softmax function used in the DPR's objective function. The temperature parameter, typically denoted as $\\tau$, adjusts the softmax function's concentration level, allowing the model to be more or less sensitive to hard negatives.\n",
    "\n",
    "By modifying the softmax function in the DPR objective function as follows, we can control the penalty on hard negatives:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(q_i, p_i^+, p_{i, 1}^-, \\cdots, p_{i,n}^-) = -\\log \\frac{\\exp(\\text{sim}(q_i, p_i^+) / \\tau)}{\\exp(\\text{sim}(q_i, p_i^+) / \\tau) + \\sum_{j=1}^n \\exp(\\text{sim}(q_i, p_{i, j}^-) / \\tau)}\n",
    "$$\n",
    "\n",
    "A higher temperature makes the softmax function smoother, thereby reducing the penalty on hard negatives. Conversely, a lower temperature sharpens the softmax function, increasing the penalty on hard negatives.\n",
    "\n",
    "Incorporating this temperature hyperparameter allows for more nuanced training of the DPR model. It enables the fine-tuning of the model's sensitivity to hard negatives, potentially improving the discrimination between relevant and non-relevant passages and enhancing overall retrieval performance. However, the optimal temperature value may vary depending on the specific dataset and needs to be empirically determined.\n",
    "\n",
    "Reference: [Wang, F., & Liu, H. (2021). Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 2495-2504).](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Understanding_the_Behaviour_of_Contrastive_Loss_CVPR_2021_paper.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
